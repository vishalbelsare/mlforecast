[
  {
    "objectID": "distributed.models.spark.lgb.html",
    "href": "distributed.models.spark.lgb.html",
    "title": "SparkLGBMForecast",
    "section": "",
    "text": "Wrapper of synapse.ml.lightgbm.LightGBMRegressor that adds an extract_local_model method to get a local version of the trained model and broadcast it to the workers.\n\n\nSparkLGBMForecast\n\n SparkLGBMForecast ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.models.dask.xgb.html",
    "href": "distributed.models.dask.xgb.html",
    "title": "DaskXGBForecast",
    "section": "",
    "text": "Wrapper of xgboost.dask.DaskXGBRegressor that adds a model_ property that contains the fitted model and is sent to the workers in the forecasting step.\n\n\nDaskXGBForecast\n\n DaskXGBForecast (max_depth:Optional[int]=None,\n                  max_leaves:Optional[int]=None,\n                  max_bin:Optional[int]=None,\n                  grow_policy:Optional[str]=None,\n                  learning_rate:Optional[float]=None,\n                  n_estimators:int=100, verbosity:Optional[int]=None, obje\n                  ctive:Union[str,Callable[[numpy.ndarray,numpy.ndarray],T\n                  uple[numpy.ndarray,numpy.ndarray]],NoneType]=None,\n                  booster:Optional[str]=None,\n                  tree_method:Optional[str]=None,\n                  n_jobs:Optional[int]=None, gamma:Optional[float]=None,\n                  min_child_weight:Optional[float]=None,\n                  max_delta_step:Optional[float]=None,\n                  subsample:Optional[float]=None,\n                  sampling_method:Optional[str]=None,\n                  colsample_bytree:Optional[float]=None,\n                  colsample_bylevel:Optional[float]=None,\n                  colsample_bynode:Optional[float]=None,\n                  reg_alpha:Optional[float]=None,\n                  reg_lambda:Optional[float]=None,\n                  scale_pos_weight:Optional[float]=None,\n                  base_score:Optional[float]=None, random_state:Union[nump\n                  y.random.mtrand.RandomState,int,NoneType]=None,\n                  missing:float=nan, num_parallel_tree:Optional[int]=None,\n                  monotone_constraints:Union[Dict[str,int],str,NoneType]=N\n                  one, interaction_constraints:Union[str,Sequence[Sequence\n                  [str]],NoneType]=None,\n                  importance_type:Optional[str]=None,\n                  gpu_id:Optional[int]=None,\n                  validate_parameters:Optional[bool]=None,\n                  predictor:Optional[str]=None,\n                  enable_categorical:bool=False,\n                  feature_types:Sequence[str]=None,\n                  max_cat_to_onehot:Optional[int]=None,\n                  max_cat_threshold:Optional[int]=None,\n                  eval_metric:Union[str,List[str],Callable,NoneType]=None,\n                  early_stopping_rounds:Optional[int]=None, callbacks:Opti\n                  onal[List[xgboost.callback.TrainingCallback]]=None,\n                  **kwargs:Any)\n\nImplementation of the Scikit-Learn API for XGBoost.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmax_depth\ntyping.Optional[int]\nNone\nMaximum tree depth for base learners.\n\n\nmax_leaves\ntyping.Optional[int]\nNone\nMaximum number of leaves; 0 indicates no limit.\n\n\nmax_bin\ntyping.Optional[int]\nNone\nIf using histogram-based algorithm, maximum number of bins per feature\n\n\ngrow_policy\ntyping.Optional[str]\nNone\nTree growing policy. 0: favor splitting at nodes closest to the node, i.e. growdepth-wise. 1: favor splitting at nodes with highest loss change.\n\n\nlearning_rate\ntyping.Optional[float]\nNone\nBoosting learning rate (xgb’s “eta”)\n\n\nn_estimators\nint\n100\nNumber of gradient boosted trees. Equivalent to number of boostingrounds.\n\n\nverbosity\ntyping.Optional[int]\nNone\nThe degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n\n\nobjective\ntyping.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\nNone\nSpecify the learning task and the corresponding learning objective ora custom objective function to be used (see note below).\n\n\nbooster\ntyping.Optional[str]\nNone\n\n\n\ntree_method\ntyping.Optional[str]\nNone\n\n\n\nn_jobs\ntyping.Optional[int]\nNone\nNumber of parallel threads used to run xgboost. When used with otherScikit-Learn algorithms like grid search, you may choose which algorithm toparallelize and balance the threads. Creating thread contention willsignificantly slow down both algorithms.\n\n\ngamma\ntyping.Optional[float]\nNone\n(min_split_loss) Minimum loss reduction required to make a further partition on aleaf node of the tree.\n\n\nmin_child_weight\ntyping.Optional[float]\nNone\nMinimum sum of instance weight(hessian) needed in a child.\n\n\nmax_delta_step\ntyping.Optional[float]\nNone\nMaximum delta step we allow each tree’s weight estimation to be.\n\n\nsubsample\ntyping.Optional[float]\nNone\nSubsample ratio of the training instance.\n\n\nsampling_method\ntyping.Optional[str]\nNone\nSampling method. Used only by gpu_hist tree method. - uniform: select random training instances uniformly. - gradient_based select random training instances with higher probability when the gradient and hessian are larger. (cf. CatBoost)\n\n\ncolsample_bytree\ntyping.Optional[float]\nNone\nSubsample ratio of columns when constructing each tree.\n\n\ncolsample_bylevel\ntyping.Optional[float]\nNone\nSubsample ratio of columns for each level.\n\n\ncolsample_bynode\ntyping.Optional[float]\nNone\nSubsample ratio of columns for each split.\n\n\nreg_alpha\ntyping.Optional[float]\nNone\nL1 regularization term on weights (xgb’s alpha).\n\n\nreg_lambda\ntyping.Optional[float]\nNone\nL2 regularization term on weights (xgb’s lambda).\n\n\nscale_pos_weight\ntyping.Optional[float]\nNone\nBalancing of positive and negative weights.\n\n\nbase_score\ntyping.Optional[float]\nNone\nThe initial prediction score of all instances, global bias.\n\n\nrandom_state\ntyping.Union[numpy.random.mtrand.RandomState, int, NoneType]\nNone\nRandom number seed... note:: Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n\n\nmissing\nfloat\nnan\nValue in the data which needs to be present as a missing value.\n\n\nnum_parallel_tree\ntyping.Optional[int]\nNone\n\n\n\nmonotone_constraints\ntyping.Union[typing.Dict[str, int], str, NoneType]\nNone\nConstraint of variable monotonicity. See :doc:tutorial &lt;/tutorials/monotonic&gt;for more information.\n\n\ninteraction_constraints\ntyping.Union[str, typing.Sequence[typing.Sequence[str]], NoneType]\nNone\nConstraints for interaction representing permitted interactions. Theconstraints must be specified in the form of a nested list, e.g. [[0, 1], [2,&lt;br&gt;3, 4]], where each inner list is a group of indices of features that areallowed to interact with each other. See :doc:tutorial&lt;br&gt;&lt;/tutorials/feature_interaction_constraint&gt; for more information\n\n\nimportance_type\ntyping.Optional[str]\nNone\n\n\n\ngpu_id\ntyping.Optional[int]\nNone\nDevice ordinal.\n\n\nvalidate_parameters\ntyping.Optional[bool]\nNone\nGive warnings for unknown parameter.\n\n\npredictor\ntyping.Optional[str]\nNone\nForce XGBoost to use specific predictor, available choices are [cpu_predictor,gpu_predictor].\n\n\nenable_categorical\nbool\nFalse\n.. versionadded:: 1.5.0.. note:: This parameter is experimentalExperimental support for categorical data. When enabled, cudf/pandas.DataFrameshould be used to specify categorical data type. Also, JSON/UBJSONserialization format is required.\n\n\nfeature_types\ntyping.Sequence[str]\nNone\n.. versionadded:: 1.7.0Used for specifying feature types without constructing a dataframe. See:py:class:DMatrix for details.\n\n\nmax_cat_to_onehot\ntyping.Optional[int]\nNone\n.. versionadded:: 1.6.0.. note:: This parameter is experimentalA threshold for deciding whether XGBoost should use one-hot encoding based splitfor categorical data. When number of categories is lesser than the thresholdthen one-hot encoding is chosen, otherwise the categories will be partitionedinto children nodes. Also, enable_categorical needs to be set to havecategorical feature support. See :doc:Categorical Data&lt;br&gt;&lt;/tutorials/categorical&gt; and :ref:cat-param for details.\n\n\nmax_cat_threshold\ntyping.Optional[int]\nNone\n.. versionadded:: 1.7.0.. note:: This parameter is experimentalMaximum number of categories considered for each split. Used only bypartition-based splits for preventing over-fitting. Also, enable_categoricalneeds to be set to have categorical feature support. See :doc:Categorical Data&lt;br&gt;&lt;/tutorials/categorical&gt; and :ref:cat-param for details.\n\n\neval_metric\ntyping.Union[str, typing.List[str], typing.Callable, NoneType]\nNone\n.. versionadded:: 1.6.0Metric used for monitoring the training result and early stopping. It can be astring or list of strings as names of predefined metric in XGBoost (Seedoc/parameter.rst), one of the metrics in :py:mod:sklearn.metrics, or any otheruser defined metric that looks like sklearn.metrics.If custom objective is also provided, then custom metric should implement thecorresponding reverse link function.Unlike the scoring parameter commonly used in scikit-learn, when a callableobject is provided, it’s assumed to be a cost function and by default XGBoost willminimize the result during early stopping.For advanced usage on Early stopping like directly choosing to maximize instead ofminimize, see :py:obj:xgboost.callback.EarlyStopping.See :doc:Custom Objective and Evaluation Metric &lt;/tutorials/custom_metric_obj&gt;for more... note:: This parameter replaces eval_metric in :py:meth:fit method. The old one receives un-transformed prediction regardless of whether custom objective is being used... code-block:: python from sklearn.datasets import load_diabetes from sklearn.metrics import mean_absolute_error X, y = load_diabetes(return_X_y=True) reg = xgb.XGBRegressor( tree_method=“hist”, eval_metric=mean_absolute_error, ) reg.fit(X, y, eval_set=[(X, y)])\n\n\nearly_stopping_rounds\ntyping.Optional[int]\nNone\n.. versionadded:: 1.6.0Activates early stopping. Validation metric needs to improve at least once inevery early_stopping_rounds round(s) to continue training. Requires at leastone item in eval_set in :py:meth:fit.The method returns the model from the last iteration (not the best one). Ifthere’s more than one item in eval_set, the last entry will be used for earlystopping. If there’s more than one metric in eval_metric, the last metricwill be used for early stopping.If early stopping occurs, the model will have three additional fields::py:attr:best_score, :py:attr:best_iteration and:py:attr:best_ntree_limit... note:: This parameter replaces early_stopping_rounds in :py:meth:fit method.\n\n\ncallbacks\ntyping.Optional[typing.List[xgboost.callback.TrainingCallback]]\nNone\nList of callback functions that are applied at end of each iteration.It is possible to use predefined callbacks by using:ref:Callback API &lt;callback_api&gt;... note:: States in callback are not preserved during training, which means callback objects can not be reused for multiple training sessions without reinitialization or deepcopy... code-block:: python for params in parameters_grid: # be sure to (re)initialize the callbacks before each run callbacks = [xgb.callback.LearningRateScheduler(custom_rates)] xgboost.train(params, Xy, callbacks=callbacks)\n\n\nkwargs\ntyping.Any\n\nKeyword arguments for XGBoost Booster object. Full documentation of parameterscan be found :doc:here &lt;/parameter&gt;.Attempting to set a parameter via the constructor args and **kwargsdict simultaneously will result in a TypeError... note:: **kwargs unsupported by scikit-learn **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn.\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/prediction_intervals.html",
    "href": "docs/prediction_intervals.html",
    "title": "Probabilistic forecasting",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with MLForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/prediction_intervals.html#introduction",
    "href": "docs/prediction_intervals.html#introduction",
    "title": "Probabilistic forecasting",
    "section": "Introduction",
    "text": "Introduction\nWhen we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn’t tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\nA prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\nWith MLForecast you can train sklearn models to generate point forecasts. It also takes the advantages of ConformalPrediction to generate the same point forecasts and adds them prediction intervals. By the end of this tutorial, you’ll have a good understanding of how to add probabilistic intervals to sklearn models for time series forecasting. Furthermore, you’ll also learn how to generate plots with the historical data, the point forecasts, and the prediction intervals.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the terms are often confused, prediction intervals are not the same as confidence intervals.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn practice, most prediction intervals are too narrow since models do not account for all sources of uncertainty. A discussion about this can be found here.\n\n\nOutline:\n\nInstall libraries\nLoad and explore the data\nTrain models\nPlot prediction intervals\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/prediction_intervals.html#install-libraries",
    "href": "docs/prediction_intervals.html#install-libraries",
    "title": "Probabilistic forecasting",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install mlforecast statsforecast"
  },
  {
    "objectID": "docs/prediction_intervals.html#load-and-explore-the-data",
    "href": "docs/prediction_intervals.html#load-and-explore-the-data",
    "title": "Probabilistic forecasting",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use the hourly dataset from the M4 Competition. We first need to download the data from a URL and then load it as a pandas dataframe. Notice that we’ll load the train and the test data separately. We’ll also rename the y column of the test data as y_test.\n\nimport pandas as pd \n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny_test\n\n\n\n\n0\nH1\n701\n619.0\n\n\n1\nH1\n702\n565.0\n\n\n2\nH1\n703\n532.0\n\n\n3\nH1\n704\n495.0\n\n\n4\nH1\n705\n481.0\n\n\n\n\n\n\n\nSince the goal of this notebook is to generate prediction intervals, we’ll only use the first 8 series of the dataset to reduce the total computational time.\n\nn_series = 8 \nuids = train['unique_id'].unique()[:n_series] # select first n_series of the dataset\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\nWe can plot these series using the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: bool = True. Plots the time series randomly.\nmodels: List[str]. A list with the models we want to plot.\nlevel: List[float]. A list with the prediction intervals we want to plot.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast as sf\n\nsf.plot(train, test, plot_random=False)"
  },
  {
    "objectID": "docs/prediction_intervals.html#train-models",
    "href": "docs/prediction_intervals.html#train-models",
    "title": "Probabilistic forecasting",
    "section": "Train models",
    "text": "Train models\nMLForecast can train multiple models that follow the sklearn syntax (fit and predict) on different time series efficiently.\nFor this example, we’ll use the following sklearn baseline models:\n\nLasso\nLinearRegression\nRidge\nK-Nearest Neighbors\nMultilayer Perceptron (NeuralNetwork)\n\nTo use these models, we first need to import them from sklearn and then we need to instantiate them.\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom mlforecast.utils import PredictionIntervals\nfrom sklearn.linear_model import Lasso, LinearRegression, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n# Create a list of models and instantiation parameters \nmodels = [\n    KNeighborsRegressor(),\n    Lasso(),\n    LinearRegression(),\n    MLPRegressor(),\n    Ridge(),\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\nmodels: The list of models defined in the previous step.\n\ntarget_transforms: Transformations to apply to the target before computing the features. These are restored at the forecasting step.\nlags: Lags of the target to use as features.\n\n\nmlf = MLForecast(\n    models=[Ridge(), Lasso(), LinearRegression(), KNeighborsRegressor(), MLPRegressor()],\n    target_transforms=[Differences([1])],\n    lags=[24 * (i+1) for i in range(7)],\n)\n\nNow we’re ready to generate the point forecasts and the prediction intervals. To do this, we’ll use the fit method, which takes the following arguments:\n\ndata: Series data in long format.\nid_col: Column that identifies each series. In our case, unique_id.\ntime_col: Column that identifies each timestep, its values can be timestamps or integers. In our case, ds.\ntarget_col: Column that contains the target. In our case, y.\nprediction_intervals: A PredicitonIntervals class. The class takes two parameters: n_windows and window_size. n_windows represents the number of cross-validation windows used to calibrate the intervals and window_size is the forecast horizon. The strategy will adjust the intervals for each horizon step, resulting in different widths for each step.\n\n\nmlf.fit(\n    train, \n    id_col='unique_id', \n    time_col='ds', \n    target_col='y', \n    prediction_intervals=PredictionIntervals(n_windows=10, window_size=48),\n);\n\nAfter fitting the models, we will call the predict method to generate forecasts with prediction intervals. The method takes the following arguments:\n\nhorizon: An integer that represent the forecasting horizon. In this case, we’ll forecast the next 48 hours.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nlevels = [50, 80, 95]\nforecasts = mlf.predict(48, level=levels)\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nRidge\nLasso\nLinearRegression\nKNeighborsRegressor\nMLPRegressor\nRidge-lo-95\nRidge-lo-80\nRidge-lo-50\n...\nKNeighborsRegressor-lo-50\nKNeighborsRegressor-hi-50\nKNeighborsRegressor-hi-80\nKNeighborsRegressor-hi-95\nMLPRegressor-lo-95\nMLPRegressor-lo-80\nMLPRegressor-lo-50\nMLPRegressor-hi-50\nMLPRegressor-hi-80\nMLPRegressor-hi-95\n\n\n\n\n0\nH1\n701\n612.418170\n612.418079\n612.418170\n615.2\n611.065169\n590.473256\n594.326570\n603.409944\n...\n609.45\n620.95\n627.20\n631.310\n579.061181\n588.591373\n593.258502\n628.871835\n633.538964\n643.069156\n\n\n1\nH1\n702\n552.309298\n552.308073\n552.309298\n551.6\n544.966946\n498.721501\n518.433843\n532.710850\n...\n535.85\n567.35\n569.16\n597.525\n482.386790\n491.212959\n508.546318\n581.387575\n598.720934\n607.547103\n\n\n2\nH1\n703\n494.943384\n494.943367\n494.943384\n509.6\n487.817383\n448.253304\n463.266064\n475.006125\n...\n492.70\n526.50\n530.92\n544.180\n412.149063\n420.562736\n441.928356\n533.706411\n555.072031\n563.485704\n\n\n3\nH1\n704\n462.815779\n462.815363\n462.815779\n474.6\n455.778433\n409.975219\n422.243593\n436.128272\n...\n451.80\n497.40\n510.26\n525.500\n362.899887\n372.485574\n405.706093\n505.850773\n539.071293\n548.656980\n\n\n4\nH1\n705\n440.141034\n440.140586\n440.141034\n451.6\n433.402236\n377.999588\n392.523016\n413.474795\n...\n427.40\n475.80\n488.96\n503.945\n330.501950\n337.899875\n377.052014\n489.752457\n528.904596\n536.302522\n\n\n\n\n5 rows × 37 columns\n\n\n\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "docs/prediction_intervals.html#plot-prediction-intervals",
    "href": "docs/prediction_intervals.html#plot-prediction-intervals",
    "title": "Probabilistic forecasting",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nTo plot the point and the prediction intervals, we’ll use the statsforecast.plot method again. Notice that now we also need to specify the model and the levels that we want to plot.\n\nKNeighborsRegressor\n\nsf.plot(\n    train, \n    test, \n    plot_random=False, \n    models=['KNeighborsRegressor', 'y_test'], \n    level=levels, \n    max_insample_length=48\n)\n\n\n                                                \n\n\n\n\nLasso\n\nsf.plot(\n    train, \n    test, \n    plot_random=False, \n    models=['Lasso', 'y_test'], \n    level=levels, \n    max_insample_length=48\n)\n\n\n                                                \n\n\n\n\nLineaRegression\n\nsf.plot(\n    train, \n    test, \n    plot_random=False, \n    models=['LinearRegression', 'y_test'], \n    level=levels, \n    max_insample_length=48\n)\n\n\n                                                \n\n\n\n\nMLPRegressor\n\nsf.plot(\n    train, \n    test, \n    plot_random=False, \n    models=['MLPRegressor', 'y_test'], \n    level=levels, \n    max_insample_length=48\n)\n\n\n                                                \n\n\n\n\nRidge\n\nsf.plot(\n    train, \n    test, \n    plot_random=False, \n    models=['Ridge', 'y_test'], \n    level=levels, \n    max_insample_length=48\n)\n\n\n                                                \n\n\nFrom these plots, we can conclude that the uncertainty around each forecast varies according to the model that is being used. For the same time series, one model can predict a wider range of possible future values than others."
  },
  {
    "objectID": "docs/prediction_intervals.html#references",
    "href": "docs/prediction_intervals.html#references",
    "title": "Probabilistic forecasting",
    "section": "References",
    "text": "References\n\nKamile Stankeviciute, Ahmed M. Alaa and Mihaela van der Schaar (2021). “Conformal Time-Series Forecasting”\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, The Statistical Forecasting Perspective”."
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html",
    "href": "docs/end_to_end_walkthrough.html",
    "title": "End to end walkthrough",
    "section": "",
    "text": "For this example we’ll use a subset of the M4 hourly dataset. You can find the a notebook with the full dataset here.\n\nimport random\n\nimport pandas as pd\nfrom datasetsforecast.m4 import M4\n\n\nawait M4.async_download('data', group='Hourly')\ndf, *_ = M4.load('data', 'Hourly')\nuids = df['unique_id'].unique()\nrandom.seed(0)\nsample_uids = random.choices(uids, k=4)\ndf = df[df['unique_id'].isin(sample_uids)].reset_index(drop=True)\ndf['ds'] = df['ds'].astype('int64')\ndf\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH196\n1\n11.8\n\n\n1\nH196\n2\n11.4\n\n\n2\nH196\n3\n11.1\n\n\n3\nH196\n4\n10.8\n\n\n4\nH196\n5\n10.6\n\n\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n99.0\n\n\n4028\nH413\n1005\n88.0\n\n\n4029\nH413\n1006\n47.0\n\n\n4030\nH413\n1007\n41.0\n\n\n4031\nH413\n1008\n34.0\n\n\n\n\n4032 rows × 3 columns\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#data-setup",
    "href": "docs/end_to_end_walkthrough.html#data-setup",
    "title": "End to end walkthrough",
    "section": "",
    "text": "For this example we’ll use a subset of the M4 hourly dataset. You can find the a notebook with the full dataset here.\n\nimport random\n\nimport pandas as pd\nfrom datasetsforecast.m4 import M4\n\n\nawait M4.async_download('data', group='Hourly')\ndf, *_ = M4.load('data', 'Hourly')\nuids = df['unique_id'].unique()\nrandom.seed(0)\nsample_uids = random.choices(uids, k=4)\ndf = df[df['unique_id'].isin(sample_uids)].reset_index(drop=True)\ndf['ds'] = df['ds'].astype('int64')\ndf\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH196\n1\n11.8\n\n\n1\nH196\n2\n11.4\n\n\n2\nH196\n3\n11.1\n\n\n3\nH196\n4\n10.8\n\n\n4\nH196\n5\n10.6\n\n\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n99.0\n\n\n4028\nH413\n1005\n88.0\n\n\n4029\nH413\n1006\n47.0\n\n\n4030\nH413\n1007\n41.0\n\n\n4031\nH413\n1008\n34.0\n\n\n\n\n4032 rows × 3 columns"
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#eda",
    "href": "docs/end_to_end_walkthrough.html#eda",
    "title": "End to end walkthrough",
    "section": "EDA",
    "text": "EDA\nWe’ll take a look at our series to get ideas for transformations and features.\n\nimport matplotlib.pyplot as plt\n\ndef plot(df, fname, last_n=24 * 14):\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(14, 6), gridspec_kw=dict(hspace=0.5))\n    uids = df['unique_id'].unique()\n    for i, (uid, axi) in enumerate(zip(uids, ax.flat)):\n        legend = i % 2 == 0\n        df[df['unique_id'].eq(uid)].tail(last_n).set_index('ds').plot(ax=axi, title=uid, legend=legend)\n    fig.savefig(fname, bbox_inches='tight')\n    plt.close()\n\n\nplot(df, 'figs/end_to_end_walkthrough__eda.png')\n\n\nWe can use the MLForecast.preprocess method to explore different transformations. It looks like these series have a strong seasonality on the hour of the day, so we can subtract the value from the same hour in the previous day to remove it. This can be done with the mlforecast.target_transforms.Differences transformer, which we pass through target_transforms.\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\n\n\nfcst = MLForecast(\n    models=[],  # we're not interested in modeling yet\n    freq=1,  # our series have integer timestamps, so we'll just add 1 in every timestep\n    target_transforms=[Differences([24])],\n)\nprep = fcst.preprocess(df)\nprep\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n24\nH196\n25\n0.3\n\n\n25\nH196\n26\n0.3\n\n\n26\nH196\n27\n0.1\n\n\n27\nH196\n28\n0.2\n\n\n28\nH196\n29\n0.2\n\n\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n39.0\n\n\n4028\nH413\n1005\n55.0\n\n\n4029\nH413\n1006\n14.0\n\n\n4030\nH413\n1007\n3.0\n\n\n4031\nH413\n1008\n4.0\n\n\n\n\n3936 rows × 3 columns\n\n\n\nThis has subtacted the lag 24 from each value, we can see what our series look like now.\n\nplot(prep, 'figs/end_to_end_walkthrough__differences.png')"
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#adding-features",
    "href": "docs/end_to_end_walkthrough.html#adding-features",
    "title": "End to end walkthrough",
    "section": "Adding features",
    "text": "Adding features\n\nLags\nLooks like the seasonality is gone, we can now try adding some lag features.\n\nfcst = MLForecast(\n    models=[],\n    freq=1,\n    lags=[1, 24],\n    target_transforms=[Differences([24])],    \n)\nprep = fcst.preprocess(df)\nprep\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nlag1\nlag24\n\n\n\n\n48\nH196\n49\n0.1\n0.1\n0.3\n\n\n49\nH196\n50\n0.1\n0.1\n0.3\n\n\n50\nH196\n51\n0.2\n0.1\n0.1\n\n\n51\nH196\n52\n0.1\n0.2\n0.2\n\n\n52\nH196\n53\n0.1\n0.1\n0.2\n\n\n...\n...\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n39.0\n29.0\n1.0\n\n\n4028\nH413\n1005\n55.0\n39.0\n-25.0\n\n\n4029\nH413\n1006\n14.0\n55.0\n-20.0\n\n\n4030\nH413\n1007\n3.0\n14.0\n0.0\n\n\n4031\nH413\n1008\n4.0\n3.0\n-16.0\n\n\n\n\n3840 rows × 5 columns\n\n\n\n\nprep.drop(columns=['unique_id', 'ds']).corr()['y']\n\ny        1.000000\nlag1     0.622531\nlag24   -0.234268\nName: y, dtype: float64\n\n\n\n\nLag transforms\nLag transforms are defined as a dictionary where the keys are the lags and the values are lists of functions that transform an array. These must be numba jitted functions (so that computing the features doesn’t become a bottleneck). There are some implemented in the window-ops package but you can also implement your own.\nIf the function takes two or more arguments you can either:\n\nsupply a tuple (tfm_func, arg1, arg2, …)\ndefine a new function fixing the arguments\n\n\nfrom numba import njit\nfrom window_ops.expanding import expanding_mean\nfrom window_ops.rolling import rolling_mean\n\n\n@njit\ndef rolling_mean_48(x):\n    return rolling_mean(x, window_size=48)\n\n\nfcst = MLForecast(\n    models=[],\n    freq=1,\n    target_transforms=[Differences([24])],    \n    lag_transforms={\n        1: [expanding_mean],\n        24: [(rolling_mean, 48), rolling_mean_48],\n    },\n)\nprep = fcst.preprocess(df)\nprep\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nexpanding_mean_lag1\nrolling_mean_lag24_window_size48\nrolling_mean_48_lag24\n\n\n\n\n95\nH196\n96\n0.1\n0.174648\n0.150000\n0.150000\n\n\n96\nH196\n97\n0.3\n0.173611\n0.145833\n0.145833\n\n\n97\nH196\n98\n0.3\n0.175342\n0.141667\n0.141667\n\n\n98\nH196\n99\n0.3\n0.177027\n0.141667\n0.141667\n\n\n99\nH196\n100\n0.3\n0.178667\n0.141667\n0.141667\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n39.0\n0.242084\n3.437500\n3.437500\n\n\n4028\nH413\n1005\n55.0\n0.281633\n2.708333\n2.708333\n\n\n4029\nH413\n1006\n14.0\n0.337411\n2.125000\n2.125000\n\n\n4030\nH413\n1007\n3.0\n0.351324\n1.770833\n1.770833\n\n\n4031\nH413\n1008\n4.0\n0.354018\n1.208333\n1.208333\n\n\n\n\n3652 rows × 6 columns\n\n\n\nYou can see that both approaches get to the same result, you can use whichever one you feel most comfortable with.\n\n\nDate features\nIf your time column is made of timestamps then it might make sense to extract features like week, dayofweek, quarter, etc. You can do that by passing a list of strings with pandas time/date components. You can also pass functions that will take the time column as input, as we’ll show here.\n\ndef hour_index(times):\n    return times % 24\n\nfcst = MLForecast(\n    models=[],\n    freq=1,\n    target_transforms=[Differences([24])],\n    date_features=[hour_index],\n)\nfcst.preprocess(df)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nhour_index\n\n\n\n\n24\nH196\n25\n0.3\n1\n\n\n25\nH196\n26\n0.3\n2\n\n\n26\nH196\n27\n0.1\n3\n\n\n27\nH196\n28\n0.2\n4\n\n\n28\nH196\n29\n0.2\n5\n\n\n...\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n39.0\n20\n\n\n4028\nH413\n1005\n55.0\n21\n\n\n4029\nH413\n1006\n14.0\n22\n\n\n4030\nH413\n1007\n3.0\n23\n\n\n4031\nH413\n1008\n4.0\n0\n\n\n\n\n3936 rows × 4 columns\n\n\n\n\n\nTarget transformations\nIf you want to do some transformation to your target before computing the features and then re-apply it after predicting you can use the target_transforms argument, which takes a list of transformations like the following:\n\nfrom mlforecast.target_transforms import BaseTargetTransform\n\nclass StandardScaler(BaseTargetTransform):\n    \"\"\"Standardizes the series by subtracting their mean and dividing by their standard deviation.\"\"\"\n    def fit_transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        self.norm_ = df.groupby(self.id_col)[self.target_col].agg(['mean', 'std'])\n        df = df.merge(self.norm_, on=self.id_col)\n        df[self.target_col] = (df[self.target_col] - df['mean']) / df['std']\n        df = df.drop(columns=['mean', 'std'])\n        return df\n\n    def inverse_transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df.merge(self.norm_, on=self.id_col)\n        for col in df.columns.drop([self.id_col, self.time_col, 'mean', 'std']):\n            df[col] = df[col] * df['std'] + df['mean']\n        df = df.drop(columns=['std', 'mean'])\n        return df\n\n\nfcst = MLForecast(\n    models=[],\n    freq=1,\n    lags=[1],\n    target_transforms=[StandardScaler()]\n)\nfcst.preprocess(df)\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nlag1\n\n\n\n\n1\nH196\n2\n-1.492285\n-1.382600\n\n\n2\nH196\n3\n-1.574549\n-1.492285\n\n\n3\nH196\n4\n-1.656813\n-1.574549\n\n\n4\nH196\n5\n-1.711656\n-1.656813\n\n\n5\nH196\n6\n-1.793919\n-1.711656\n\n\n...\n...\n...\n...\n...\n\n\n4027\nH413\n1004\n3.061246\n2.423809\n\n\n4028\nH413\n1005\n2.521876\n3.061246\n\n\n4029\nH413\n1006\n0.511497\n2.521876\n\n\n4030\nH413\n1007\n0.217295\n0.511497\n\n\n4031\nH413\n1008\n-0.125941\n0.217295\n\n\n\n\n4028 rows × 4 columns\n\n\n\nWe can define a naive model to test this\n\nfrom sklearn.base import BaseEstimator\n\nclass Naive(BaseEstimator):\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return X['lag1']\n\n\nfcst = MLForecast(\n    models=[Naive()],\n    freq=1,\n    lags=[1],\n    target_transforms=[StandardScaler()]\n)\nfcst.fit(df)\npreds = fcst.predict(1)\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\nNaive\n\n\n\n\n0\nH196\n1009\n16.8\n\n\n1\nH256\n1009\n13.4\n\n\n2\nH381\n1009\n207.0\n\n\n3\nH413\n1009\n34.0\n\n\n\n\n\n\n\nWe compare this with the last values of our serie\n\nlast_vals = df.groupby('unique_id').tail(1)\nlast_vals\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n1007\nH196\n1008\n16.8\n\n\n2015\nH256\n1008\n13.4\n\n\n3023\nH381\n1008\n207.0\n\n\n4031\nH413\n1008\n34.0\n\n\n\n\n\n\n\n\nimport numpy as np\n\nnp.testing.assert_allclose(preds['Naive'], last_vals['y'])"
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#training",
    "href": "docs/end_to_end_walkthrough.html#training",
    "title": "End to end walkthrough",
    "section": "Training",
    "text": "Training\nOnce you’ve decided the features, transformations and models that you want to use you can use the MLForecast.fit method instead, which will do the preprocessing and then train the models. The models can be specified as a list (which will name them by using their class name and an index if there are repeated classes) or as a dictionary where the keys are the names you want to give to the models, i.e. the name of the column that will hold their predictions, and the values are the models themselves.\n\nimport lightgbm as lgb\n\n\nlgb_params = {\n    'verbosity': -1,\n    'num_leaves': 512,\n}\n\nfcst = MLForecast(\n    models={\n        'avg': lgb.LGBMRegressor(**lgb_params),\n        'q75': lgb.LGBMRegressor(**lgb_params, objective='quantile', alpha=0.75),\n        'q25': lgb.LGBMRegressor(**lgb_params, objective='quantile', alpha=0.25),\n    },\n    freq=1,\n    target_transforms=[Differences([24])],\n    lags=[1, 24],\n    lag_transforms={\n        1: [expanding_mean],\n        24: [(rolling_mean, 48)],\n    },\n    date_features=[hour_index],\n)\nfcst.fit(df)\n\nMLForecast(models=[avg, q75, q25], freq=1, lag_features=['lag1', 'lag24', 'expanding_mean_lag1', 'rolling_mean_lag24_window_size48'], date_features=[&lt;function hour_index&gt;], num_threads=1)\n\n\nThis computed the features and trained three different models using them. We can now compute our forecasts."
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#forecasting",
    "href": "docs/end_to_end_walkthrough.html#forecasting",
    "title": "End to end walkthrough",
    "section": "Forecasting",
    "text": "Forecasting\n\npreds = fcst.predict(48)\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\navg\nq75\nq25\n\n\n\n\n0\nH196\n1009\n16.295257\n16.385859\n16.320666\n\n\n1\nH196\n1010\n15.910282\n16.012728\n15.856905\n\n\n2\nH196\n1011\n15.728367\n15.784867\n15.656658\n\n\n3\nH196\n1012\n15.468414\n15.503223\n15.401462\n\n\n4\nH196\n1013\n15.081279\n15.163606\n15.048576\n\n\n...\n...\n...\n...\n...\n...\n\n\n187\nH413\n1052\n100.450617\n116.461898\n52.276952\n\n\n188\nH413\n1053\n88.426800\n114.257158\n50.866960\n\n\n189\nH413\n1054\n59.675737\n89.672526\n16.440738\n\n\n190\nH413\n1055\n57.580356\n84.680943\n14.248400\n\n\n191\nH413\n1056\n42.669879\n52.000559\n12.440984\n\n\n\n\n192 rows × 5 columns\n\n\n\n\nimport pandas as pd\n\n\nplot(pd.concat([df, preds]), 'figs/end_to_end_walkthrough__predictions.png', last_n=24 * 7)"
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#updating-series-values",
    "href": "docs/end_to_end_walkthrough.html#updating-series-values",
    "title": "End to end walkthrough",
    "section": "Updating series’ values",
    "text": "Updating series’ values\nAfter you’ve trained a forecast object you can save it and load it to use later using pickle or cloudpickle. If by the time you want to use it you already know the following values of the target you can use the MLForecast.ts.update method to incorporate these, which will allow you to use these new values when computing predictions.\n\nIf no new values are provided for a serie that’s currently stored, only the previous ones are kept.\nIf new series are included they are added to the existing ones.\n\n\nfcst = MLForecast(\n    models=[Naive()],\n    freq=1,\n    lags=[1, 2, 3],\n)\nfcst.fit(df)\nfcst.predict(1)\n\n\n\n\n\n\n\n\nunique_id\nds\nNaive\n\n\n\n\n0\nH196\n1009\n16.8\n\n\n1\nH256\n1009\n13.4\n\n\n2\nH381\n1009\n207.0\n\n\n3\nH413\n1009\n34.0\n\n\n\n\n\n\n\n\nnew_values = pd.DataFrame({\n    'unique_id': ['H196', 'H256'],\n    'ds': [1009, 1009],\n    'y': [17.0, 14.0],\n})\nfcst.ts.update(new_values)\npreds = fcst.predict(1)\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\nNaive\n\n\n\n\n0\nH196\n1010\n17.0\n\n\n1\nH256\n1010\n14.0\n\n\n2\nH381\n1009\n207.0\n\n\n3\nH413\n1009\n34.0"
  },
  {
    "objectID": "docs/end_to_end_walkthrough.html#estimating-model-performance",
    "href": "docs/end_to_end_walkthrough.html#estimating-model-performance",
    "title": "End to end walkthrough",
    "section": "Estimating model performance",
    "text": "Estimating model performance\n\nCross validation\nIn order to get an estimate of how well our model will be when predicting future data we can perform cross validation, which consist on training a few models independently on different subsets of the data, using them to predict a validation set and measuring their performance.\nSince our data depends on time, we make our splits by removing the last portions of the series and using them as validation sets. This process is implemented in MLForecast.cross_validation.\n\nfcst = MLForecast(\n    models=lgb.LGBMRegressor(**lgb_params),\n    freq=1,\n    target_transforms=[Differences([24])],\n    lags=[1, 24],\n    lag_transforms={\n        1: [expanding_mean],\n        24: [(rolling_mean, 48)],\n    },\n    date_features=[hour_index],\n)\ncv_result = fcst.cross_validation(\n    df,\n    n_windows=4,  # number of models to train/splits to perform\n    window_size=48,  # length of the validation set in each window\n)\ncv_result\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\n\n\n\n\n0\nH196\n817\n816\n15.3\n15.383165\n\n\n1\nH196\n818\n816\n14.9\n14.923219\n\n\n2\nH196\n819\n816\n14.6\n14.667834\n\n\n3\nH196\n820\n816\n14.2\n14.275964\n\n\n4\nH196\n821\n816\n13.9\n13.973491\n\n\n...\n...\n...\n...\n...\n...\n\n\n187\nH413\n1004\n960\n99.0\n65.644823\n\n\n188\nH413\n1005\n960\n88.0\n71.717097\n\n\n189\nH413\n1006\n960\n47.0\n76.704377\n\n\n190\nH413\n1007\n960\n41.0\n53.446638\n\n\n191\nH413\n1008\n960\n34.0\n54.902634\n\n\n\n\n768 rows × 5 columns\n\n\n\n\nplot(cv_result.drop(columns='cutoff'), 'figs/end_to_end_walkthrough__cv.png')\n\n\nWe can compute the RMSE on each split.\n\ndef evaluate_cv(df):\n    return df['y'].sub(df['LGBMRegressor']).pow(2).groupby(df['cutoff']).mean().pow(0.5)\n\nsplit_rmse = evaluate_cv(cv_result)\nsplit_rmse\n\ncutoff\n816    29.418172\n864    34.257598\n912    13.145763\n960    35.066261\ndtype: float64\n\n\nAnd the average RMSE across splits.\n\nsplit_rmse.mean()\n\n27.971948676289266\n\n\nYou can quickly try different features and evaluate them this way. We can try removing the differencing and using an exponentially weighted average of the lag 1 instead of the expanding mean.\n\nfrom window_ops.ewm import ewm_mean\n\n\nfcst = MLForecast(\n    models=lgb.LGBMRegressor(**lgb_params),\n    freq=1,\n    lags=[1, 24],\n    lag_transforms={\n        1: [(ewm_mean, 0.5)],\n        24: [(rolling_mean, 48)],      \n    },\n    date_features=[hour_index],    \n)\ncv_result2 = fcst.cross_validation(\n    df,\n    n_windows=4,\n    window_size=48,\n)\nevaluate_cv(cv_result2).mean()\n\n25.87444576840234\n\n\n\n\nLightGBMCV\nIn the same spirit of estimating our model’s performance, LightGBMCV allows us to train a few LightGBM models on different partitions of the data. The main differences with MLForecast.cross_validation are:\n\nIt can only train LightGBM models.\nIt trains all models simultaneously and gives us per-iteration averages of the errors across the complete forecasting window, which allows us to find the best iteration.\n\n\nfrom mlforecast.lgb_cv import LightGBMCV\n\n\ncv = LightGBMCV(\n    freq=1,\n    target_transforms=[Differences([24])],\n    lags=[1, 24],\n    lag_transforms={\n        1: [expanding_mean],\n        24: [(rolling_mean, 48)],\n    },\n    date_features=[hour_index],\n    num_threads=2,\n)\ncv_hist = cv.fit(\n    df,\n    n_windows=4,\n    window_size=48,\n    params=lgb_params,\n    eval_every=5,\n    early_stopping_evals=5,    \n    compute_cv_preds=True,\n)\n\n[5] mape: 0.158639\n[10] mape: 0.163739\n[15] mape: 0.161535\n[20] mape: 0.169491\n[25] mape: 0.163690\n[30] mape: 0.164198\nEarly stopping at round 30\nUsing best iteration: 5\n\n\nAs you can see this gives us the error by iteration (controlled by the eval_every argument) and performs early stopping (which can be configured with early_stopping_evals and early_stopping_pct). If you set compute_cv_preds=True the out-of-fold predictions are computed using the best iteration found and are saved in the cv_preds_ attribute.\n\ncv.cv_preds_\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nBooster\nwindow\n\n\n\n\n0\nH196\n817\n15.3\n15.473182\n0\n\n\n1\nH196\n818\n14.9\n15.038571\n0\n\n\n2\nH196\n819\n14.6\n14.849409\n0\n\n\n3\nH196\n820\n14.2\n14.448379\n0\n\n\n4\nH196\n821\n13.9\n14.148379\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n187\nH413\n1004\n99.0\n61.425396\n3\n\n\n188\nH413\n1005\n88.0\n62.886890\n3\n\n\n189\nH413\n1006\n47.0\n57.886890\n3\n\n\n190\nH413\n1007\n41.0\n38.849009\n3\n\n\n191\nH413\n1008\n34.0\n44.720562\n3\n\n\n\n\n768 rows × 5 columns\n\n\n\n\nplot(cv.cv_preds_.drop(columns='window'), 'figs/end_to_end_walkthrough__lgbcv.png')\n\n\n\nYou can use this class to quickly try different configurations of features and hyperparameters. Once you’ve found a combination that works you can train a model with those features and hyperparameters on all the data by creating an MLForecast object from the LightGBMCV one as follows:\n\nfinal_fcst = MLForecast.from_cv(cv)\nfinal_fcst.fit(df)\npreds = final_fcst.predict(48)\nplot(pd.concat([df, preds]), 'figs/end_to_end_walkthrough__final_forecast.png')"
  },
  {
    "objectID": "docs/install.html",
    "href": "docs/install.html",
    "title": "Install",
    "section": "",
    "text": "To install the latest release of mlforecast from PyPI you just have to run the following in a terminal:\npip install mlforecast\n\n\n\nIf you want a specific version you can include a filter, for example:\n\npip install \"mlforecast==0.3.0\" to install the 0.3.0 version\npip install \"mlforecast&lt;0.4.0\" to install any version prior to 0.4.0\n\n\n\n\nIf you want to perform distributed training you have to include the dask extra:\npip install \"mlforecast[dask]\"\nand also either LightGBM or XGBoost.\n\n\n\n\n\n\nThe mlforecast package is also published to conda-forge, which you can install by running the following in a terminal:\nconda install -c conda-forge mlforecast\nNote that this happens about a day later after it is published to PyPI, so you may have to wait to get the latest release.\n\n\n\nIf you want a specific version you can include a filter, for example:\n\nconda install -c conda-forge \"mlforecast==0.3.0\" to install the 0.3.0 version\nconda install -c conda-forge \"mlforecast&lt;0.4.0\" to install any version prior to 0.4.0\n\n\n\n\nIf you want to perform distributed training you also have to install dask:\nconda install -c conda-forge dask\nand also either LightGBM or XGBoost.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/install.html#released-versions",
    "href": "docs/install.html#released-versions",
    "title": "Install",
    "section": "",
    "text": "To install the latest release of mlforecast from PyPI you just have to run the following in a terminal:\npip install mlforecast\n\n\n\nIf you want a specific version you can include a filter, for example:\n\npip install \"mlforecast==0.3.0\" to install the 0.3.0 version\npip install \"mlforecast&lt;0.4.0\" to install any version prior to 0.4.0\n\n\n\n\nIf you want to perform distributed training you have to include the dask extra:\npip install \"mlforecast[dask]\"\nand also either LightGBM or XGBoost.\n\n\n\n\n\n\nThe mlforecast package is also published to conda-forge, which you can install by running the following in a terminal:\nconda install -c conda-forge mlforecast\nNote that this happens about a day later after it is published to PyPI, so you may have to wait to get the latest release.\n\n\n\nIf you want a specific version you can include a filter, for example:\n\nconda install -c conda-forge \"mlforecast==0.3.0\" to install the 0.3.0 version\nconda install -c conda-forge \"mlforecast&lt;0.4.0\" to install any version prior to 0.4.0\n\n\n\n\nIf you want to perform distributed training you also have to install dask:\nconda install -c conda-forge dask\nand also either LightGBM or XGBoost."
  },
  {
    "objectID": "docs/install.html#development-version",
    "href": "docs/install.html#development-version",
    "title": "Install",
    "section": "Development version",
    "text": "Development version\nIf you want to try out a new feature that hasn’t made it into a release yet you have the following options:\n\nInstall from github: pip install git+https://github.com/Nixtla/mlforecast\nClone and install:\n\ngit clone https://github.com/Nixtla/mlforecast\npip install mlforecast\n\n\nwhich will install the version from the current main branch."
  },
  {
    "objectID": "docs/cross_validation.html",
    "href": "docs/cross_validation.html",
    "title": "Cross validation",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with MLForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/cross_validation.html#introduction",
    "href": "docs/cross_validation.html#introduction",
    "title": "Cross validation",
    "section": "Introduction",
    "text": "Introduction\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it.\n\nMLForecast has an implementation of time series cross-validation that is fast and easy to use. This implementation makes cross-validation a efficient operation, which makes it less time-consuming. In this notebook, we’ll use it on a subset of the M4 Competition hourly dataset.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nPerform time series cross-validation\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/cross_validation.html#install-libraries",
    "href": "docs/cross_validation.html#install-libraries",
    "title": "Cross validation",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have MLForecast already installed. If not, check this guide for instructions on how to install MLForecast.\nInstall the necessary packages with pip install mlforecast.\n\npip install mlforecast lightgbm\n\n\nfrom mlforecast import MLForecast # required to instantiate MLForecast object and use cross-validation method"
  },
  {
    "objectID": "docs/cross_validation.html#load-and-explore-the-data",
    "href": "docs/cross_validation.html#load-and-explore-the-data",
    "title": "Cross validation",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nAs stated in the introduction, we’ll use the M4 Competition hourly dataset. We’ll first import the data from an URL using pandas.\n\nimport pandas as pd \n\nY_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.csv') # load the data \nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThe input to MLForecast is a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int, or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThe data in this example already has this format, so no changes are needed.\nWe can plot the time series we’ll work with using the following function.\n\nimport matplotlib.pyplot as plt\n\ndef plot(df, fname, last_n=24 * 14):\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(14, 6), gridspec_kw=dict(hspace=0.5))\n    uids = df['unique_id'].unique()\n    for i, (uid, axi) in enumerate(zip(uids, ax.flat)):\n        legend = i % 2 == 0\n        df[df['unique_id'].eq(uid)].tail(last_n).set_index('ds').plot(ax=axi, title=uid, legend=legend)\n    fig.savefig(fname, bbox_inches='tight')\n    plt.close()\n\n\nplot(Y_df, '../figs/cross_validation__series.png')"
  },
  {
    "objectID": "docs/cross_validation.html#train-model",
    "href": "docs/cross_validation.html#train-model",
    "title": "Cross validation",
    "section": "Train model",
    "text": "Train model\nFor this example, we’ll use LightGBM. We first need to import it and then we need to instantiate a new MLForecast object.\nThe MLForecast object has the following parameters:\n\nmodels: a list of sklearn-like (fit and predict) models.\nfreq: a string indicating the frequency of the data. See panda’s available frequencies.\ntarget_transforms: Transformations to apply to the target before computing the features. These are restored at the forecasting step.\nlags: Lags of the target to use as features.\n\nIn this example, we are only using differences and lags to produce features. See the full documentation to see all available features.\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame df.\n\nimport lightgbm as lgb\nfrom mlforecast.target_transforms import Differences\n\nmodels = [lgb.LGBMRegressor()]\n\nmlf = MLForecast(\n    models = models, \n    freq = 1,# our series have integer timestamps, so we'll just add 1 in every timeste, \n    target_transforms=[Differences([24])],\n    lags=range(1, 25, 1)\n)"
  },
  {
    "objectID": "docs/cross_validation.html#perform-time-series-cross-validation",
    "href": "docs/cross_validation.html#perform-time-series-cross-validation",
    "title": "Cross validation",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nOnce the MLForecast object has been instantiated, we can use the cross_validation method, which takes the following arguments:\n\ndata: training data frame with MLForecast format\nwindow_size (int): represents the h steps into the future that will be forecasted\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\nid_col: identifies each time series.\ntime_col: indetifies the temporal column of the time series.\ntarget_col: identifies the column to model.\n\nFor this particular example, we’ll use 3 windows of 24 hours.\n\ncrossvalidation_df = mlf.cross_validation(\n    data=Y_df,\n    window_size=24,\n    n_windows=3,\n)\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: identifies each time series.\nds: datestamp or temporal index.\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\n\n\n\n\n0\nH1\n677\n676\n691.0\n673.703191\n\n\n1\nH1\n678\n676\n618.0\n552.306270\n\n\n2\nH1\n679\n676\n563.0\n541.778027\n\n\n3\nH1\n680\n676\n529.0\n502.778027\n\n\n4\nH1\n681\n676\n504.0\n480.778027\n\n\n\n\n\n\n\nWe’ll now plot the forecast for each cutoff period.\n\ndef plot_cv(df, df_cv, uid, fname, last_n=24 * 14):\n    cutoffs = df_cv.query('unique_id == @uid')['cutoff'].unique()\n    fig, ax = plt.subplots(nrows=len(cutoffs), ncols=1, figsize=(14, 6), gridspec_kw=dict(hspace=0.8))\n    for cutoff, axi in zip(cutoffs, ax.flat):\n        df.query('unique_id == @uid').tail(last_n).set_index('ds').plot(ax=axi, title=uid, y='y')\n        df_cv.query('unique_id == @uid & cutoff == @cutoff').set_index('ds').plot(ax=axi, title=uid, y='LGBMRegressor')\n    fig.savefig(fname, bbox_inches='tight')\n    plt.close()\n\n\nplot_cv(Y_df, crossvalidation_df, 'H1', '../figs/cross_validation__predictions.png')\n\n\nNotice that in each cutoff period, we generated a forecast for the next 24 hours using only the data y before said period."
  },
  {
    "objectID": "docs/cross_validation.html#evaluate-results",
    "href": "docs/cross_validation.html#evaluate-results",
    "title": "Cross validation",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we’ll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\npip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\n\nThe forecasts, in this case, LGBMRegressor.\n\nIn this case we will compute the rmse per time series and cutoff and then we will take the mean of the results.\n\ncv_rmse = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(lambda df: rmse(df['y'], df['LGBMRegressor'])).mean()\nprint(\"RMSE using cross-validation: \", cv_rmse)\n\nRMSE using cross-validation:  249.90517171185527\n\n\nThis measure should better reflect the predictive abilities of our model, since it used different time periods to test its accuracy."
  },
  {
    "objectID": "docs/cross_validation.html#references",
    "href": "docs/cross_validation.html#references",
    "title": "Cross validation",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Time series cross-validation”."
  },
  {
    "objectID": "distributed.models.ray.xgb.html",
    "href": "distributed.models.ray.xgb.html",
    "title": "RayXGBForecast",
    "section": "",
    "text": "Wrapper of xgboost.ray.RayXGBRegressor that adds a model_ property that contains the fitted model and is sent to the workers in the forecasting step.\n\n\nRayXGBForecast\n\n RayXGBForecast (objective:Union[str,Callable[[numpy.ndarray,numpy.ndarray\n                 ],Tuple[numpy.ndarray,numpy.ndarray]],NoneType]='reg:squa\n                 rederror', **kwargs:Any)\n\nImplementation of the scikit-learn API for Ray-distributed XGBoost regression.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobjective\ntyping.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\nreg:squarederror\nSpecify the learning task and the corresponding learning objective ora custom objective function to be used (see note below).\n\n\nkwargs\ntyping.Any\n\nKeyword arguments for XGBoost Booster object. Full documentation of parameterscan be found :doc:here &lt;/parameter&gt;.Attempting to set a parameter via the constructor args and **kwargsdict simultaneously will result in a TypeError... note:: **kwargs unsupported by scikit-learn **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn... note:: Custom objective function A custom objective function can be provided for the objective parameter. In this case, it should have the signature objective(y_true, y_pred) -&gt; grad, hess: y_true: array_like of shape [n_samples] The target values y_pred: array_like of shape [n_samples] The predicted values grad: array_like of shape [n_samples] The value of the gradient for each sample point. hess: array_like of shape [n_samples] The value of the second derivative for each sample point\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.models.ray.lgb.html",
    "href": "distributed.models.ray.lgb.html",
    "title": "RayLGBMForecast",
    "section": "",
    "text": "Wrapper of lightgbm.ray.RayLGBMRegressor that adds a model_ property that contains the fitted booster and is sent to the workers to in the forecasting step.\n\n\nRayLGBMForecast\n\n RayLGBMForecast (boosting_type:str='gbdt', num_leaves:int=31,\n                  max_depth:int=-1, learning_rate:float=0.1,\n                  n_estimators:int=100, subsample_for_bin:int=200000, obje\n                  ctive:Union[str,Callable[[Optional[numpy.ndarray],numpy.\n                  ndarray],Tuple[numpy.ndarray,numpy.ndarray]],Callable[[O\n                  ptional[numpy.ndarray],numpy.ndarray,Optional[numpy.ndar\n                  ray]],Tuple[numpy.ndarray,numpy.ndarray]],Callable[[Opti\n                  onal[numpy.ndarray],numpy.ndarray,Optional[numpy.ndarray\n                  ],Optional[numpy.ndarray]],Tuple[numpy.ndarray,numpy.nda\n                  rray]],NoneType]=None,\n                  class_weight:Union[Dict,str,NoneType]=None,\n                  min_split_gain:float=0.0, min_child_weight:float=0.001,\n                  min_child_samples:int=20, subsample:float=1.0,\n                  subsample_freq:int=0, colsample_bytree:float=1.0,\n                  reg_alpha:float=0.0, reg_lambda:float=0.0, random_state:\n                  Union[int,numpy.random.mtrand.RandomState,NoneType]=None\n                  , n_jobs:Optional[int]=None,\n                  importance_type:str='split', **kwargs)\n\nPublicAPI (beta): This API is in beta and may change before becoming stable.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nixtla     ",
    "section": "",
    "text": "mlforecast is a framework to perform time series forecasting using machine learning models, with the option to scale to massive amounts of data using remote clusters.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Nixtla     ",
    "section": "Install",
    "text": "Install\n\nPyPI\npip install mlforecast\nIf you want to perform distributed training, you can instead use pip install \"mlforecast[distributed]\", which will also install dask. Note that you’ll also need to install either LightGBM or XGBoost.\n\n\nconda-forge\nconda install -c conda-forge mlforecast\nNote that this installation comes with the required dependencies for the local interface. If you want to perform distributed training, you must install dask (conda install -c conda-forge dask) and either LightGBM or XGBoost."
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Nixtla     ",
    "section": "Quick Start",
    "text": "Quick Start\nMinimal Example\nimport lightgbm as lgb\n\nfrom mlforecast import MLForecast\nfrom sklearn.linear_model import LinearRegression\n\nmlf = MLForecast(\n    models = [LinearRegression(), lgb.LGBMRegressor()],\n    lags=[1, 12],\n    freq = 'M'\n)\nmlf.fit(df)\nmlf.predict(12)\nGet Started with this quick guide.\nFollow this end-to-end walkthrough for best practices."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Nixtla     ",
    "section": "Why?",
    "text": "Why?\nCurrent Python alternatives for machine learning models are slow, inaccurate and don’t scale well. So we created a library that can be used to forecast in production environments. MLForecast includes efficient feature engineering to train any machine learning model (with fit and predict methods such as sklearn) to fit millions of time series."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Nixtla     ",
    "section": "Features",
    "text": "Features\n\nFastest implementations of feature engineering for time series forecasting in Python.\nOut-of-the-box compatibility with Spark, Dask, and Ray.\nProbabilistic Forecasting with Conformal Prediction.\nSupport for exogenous variables and static covariates.\nFamiliar sklearn syntax: .fit and .predict.\n\nMissing something? Please open an issue or write us in"
  },
  {
    "objectID": "index.html#examples-and-guides",
    "href": "index.html#examples-and-guides",
    "title": "Nixtla     ",
    "section": "Examples and Guides",
    "text": "Examples and Guides\n📚 End to End Walkthrough: model training, evaluation and selection for multiple time series.\n🔎 Probabilistic Forecasting: use Conformal Prediction to produce prediciton intervals.\n👩‍🔬 Cross Validation: robust model’s performance evaluation.\n🔌 Predict Demand Peaks: electricity load forecasting for detecting daily peaks and reducing electric bills.\n📈 Transfer Learning: pretrain a model using a set of time series and then predict another one using that pretrained model.\n🌡️ Distributed Training: use a Dask cluster to train models at scale."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Nixtla     ",
    "section": "How to use",
    "text": "How to use\nThe following provides a very basic overview, for a more detailed description see the documentation.\n\nData setup\nStore your time series in a pandas dataframe in long format, that is, each row represents an observation for a specific serie and timestamp.\n\nfrom mlforecast.utils import generate_daily_series\n\nseries = generate_daily_series(\n    n_series=20,\n    max_length=100,\n    n_static_features=1,\n    static_as_categorical=False,\n    with_trend=True\n)\nseries.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\n\n\n\n\n0\nid_00\n2000-01-01\n1.751917\n72\n\n\n1\nid_00\n2000-01-02\n9.196715\n72\n\n\n2\nid_00\n2000-01-03\n18.577788\n72\n\n\n3\nid_00\n2000-01-04\n24.520646\n72\n\n\n4\nid_00\n2000-01-05\n33.418028\n72\n\n\n\n\n\n\n\n\n\nModels\nNext define your models. If you want to use the local interface this can be any regressor that follows the scikit-learn API. For distributed training there are LGBMForecast and XGBForecast.\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodels = [\n    lgb.LGBMRegressor(),\n    xgb.XGBRegressor(),\n    RandomForestRegressor(random_state=0),\n]\n\n\n\nForecast object\nNow instantiate a MLForecast object with the models and the features that you want to use. The features can be lags, transformations on the lags and date features. The lag transformations are defined as numba jitted functions that transform an array, if they have additional arguments you can either supply a tuple (transform_func, arg1, arg2, …) or define new functions fixing the arguments. You can also define differences to apply to the series before fitting that will be restored when predicting.\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom numba import njit\nfrom window_ops.expanding import expanding_mean\nfrom window_ops.rolling import rolling_mean\n\n\n@njit\ndef rolling_mean_28(x):\n    return rolling_mean(x, window_size=28)\n\n\nfcst = MLForecast(\n    models=models,\n    freq='D',\n    lags=[7, 14],\n    lag_transforms={\n        1: [expanding_mean],\n        7: [rolling_mean_28]\n    },\n    date_features=['dayofweek'],\n    target_transforms=[Differences([1])],\n)\n\n\n\nTraining\nTo compute the features and train the models call fit on your Forecast object.\n\nfcst.fit(series)\n\nMLForecast(models=[LGBMRegressor, XGBRegressor, RandomForestRegressor], freq=&lt;Day&gt;, lag_features=['lag7', 'lag14', 'expanding_mean_lag1', 'rolling_mean_28_lag7'], date_features=['dayofweek'], num_threads=1)\n\n\n\n\nPredicting\nTo get the forecasts for the next n days call predict(n) on the forecast object. This will automatically handle the updates required by the features using a recursive strategy.\n\npredictions = fcst.predict(14)\npredictions\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\nXGBRegressor\nRandomForestRegressor\n\n\n\n\n0\nid_00\n2000-04-04\n69.082830\n67.761337\n68.226556\n\n\n1\nid_00\n2000-04-05\n75.706024\n74.588699\n75.484774\n\n\n2\nid_00\n2000-04-06\n82.222473\n81.058289\n82.853684\n\n\n3\nid_00\n2000-04-07\n89.577638\n88.735947\n90.351212\n\n\n4\nid_00\n2000-04-08\n44.149095\n44.981384\n46.291173\n\n\n...\n...\n...\n...\n...\n...\n\n\n275\nid_19\n2000-03-23\n30.151270\n31.814825\n32.592799\n\n\n276\nid_19\n2000-03-24\n31.418104\n32.653374\n33.563294\n\n\n277\nid_19\n2000-03-25\n32.843567\n33.586033\n34.530912\n\n\n278\nid_19\n2000-03-26\n34.127210\n34.541473\n35.507559\n\n\n279\nid_19\n2000-03-27\n34.329202\n35.450943\n36.425001\n\n\n\n\n280 rows × 5 columns\n\n\n\n\n\nVisualize results\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), gridspec_kw=dict(hspace=0.3))\nfor i, (uid, axi) in enumerate(zip(series['unique_id'].unique(), ax.flat)):\n    fltr = lambda df: df['unique_id'].eq(uid)\n    pd.concat([series.loc[fltr, ['ds', 'y']], predictions.loc[fltr]]).set_index('ds').plot(ax=axi)\n    axi.set(title=uid, xlabel=None)\n    if i % 2 == 0:\n        axi.legend().remove()\n    else:\n        axi.legend(bbox_to_anchor=(1.01, 1.0))\nfig.savefig('figs/index.png', bbox_inches='tight')\nplt.close()"
  },
  {
    "objectID": "index.html#sample-notebooks",
    "href": "index.html#sample-notebooks",
    "title": "Nixtla     ",
    "section": "Sample notebooks",
    "text": "Sample notebooks\n\nm5\nm4\nm4-cv"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Nixtla     ",
    "section": "How to contribute",
    "text": "How to contribute\nSee CONTRIBUTING.md."
  },
  {
    "objectID": "grouped_array.html",
    "href": "grouped_array.html",
    "title": "mlforecast",
    "section": "",
    "text": "GroupedArray\n\n GroupedArray (data:numpy.ndarray, indptr:numpy.ndarray)\n\nArray made up of different groups. Can be thought of (and iterated) as a list of arrays.\nAll the data is stored in a single 1d array data. The indices for the group boundaries are stored in another 1d array indptr.\n\nfrom fastcore.test import test_eq, test_fail\n\n\n# The `GroupedArray` is used internally for storing the series values and performing transformations.\ndata = np.arange(10, dtype=np.float32)\nindptr = np.array([0, 2, 10])  # group 1: [0, 1], group 2: [2..9]\nga = GroupedArray(data, indptr)\ntest_eq(len(ga), 2)\ntest_eq(str(ga), 'GroupedArray(ndata=10, ngroups=2)')\n\n\n# Iterate through the groups\nga_iter = iter(ga)\nnp.testing.assert_equal(next(ga_iter), np.array([0, 1]))\nnp.testing.assert_equal(next(ga_iter), np.arange(2, 10))\n\n\n# Take the last two observations from every group\nlast_2 = ga.take_from_groups(slice(-2, None))\nnp.testing.assert_equal(last_2.data, np.array([0, 1, 8, 9]))\nnp.testing.assert_equal(last_2.indptr, np.array([0, 2, 4]))\n\n\n# Take the last four observations from every group. Note that since group 1 only has two elements, only these are returned.\nlast_4 = ga.take_from_groups(slice(-4, None))\nnp.testing.assert_equal(last_4.data, np.array([0, 1, 6, 7, 8, 9]))\nnp.testing.assert_equal(last_4.indptr, np.array([0, 2, 6]))\n\n\n# The groups are [0, 1], [2, ..., 9]. expand_target(2) should take rolling pairs of them and fill with nans when there aren't enough\nnp.testing.assert_equal(\n    ga.expand_target(2),\n    np.array([\n        [0, 1],\n        [1, np.nan],\n        [2, 3],\n        [3, 4],\n        [4, 5],\n        [5, 6],\n        [6, 7],\n        [7, 8],\n        [8, 9],\n        [9, np.nan]\n    ])\n)\n\n\n# try to append new values that don't match the number of groups\ntest_fail(lambda: ga.append(np.array([1., 2., 3.])), contains='new must be of size 2')\n\n\n# __setitem__\nnew_vals = np.array([10, 11])\nga[0] = new_vals\nnp.testing.assert_equal(ga.data, np.append(new_vals, np.arange(2, 10)))\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "import copy\n\nfrom nbdev import show_doc\nfrom fastcore.test import test_eq, test_fail, test_warns\nfrom window_ops.expanding import expanding_mean\nfrom window_ops.rolling import rolling_mean\nfrom window_ops.shift import shift_array\n\nfrom mlforecast.target_transforms import LocalStandardScaler\nfrom mlforecast.utils import generate_daily_series, generate_prices_for_series\nGive us a ⭐ on Github"
  },
  {
    "objectID": "core.html#data-format",
    "href": "core.html#data-format",
    "title": "Core",
    "section": "Data format",
    "text": "Data format\nThe required input format is a dataframe with at least the following columns: * unique_id with a unique identifier for each time serie * ds with the datestamp and a column * y with the values of the serie.\nEvery other column is considered a static feature unless stated otherwise in TimeSeries.fit\n\nseries = generate_daily_series(20, n_static_features=2)\nseries\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nstatic_1\n\n\n\n\n0\nid_00\n2000-01-01\n0.740453\n27\n53\n\n\n1\nid_00\n2000-01-02\n3.595262\n27\n53\n\n\n2\nid_00\n2000-01-03\n6.895835\n27\n53\n\n\n3\nid_00\n2000-01-04\n8.499450\n27\n53\n\n\n4\nid_00\n2000-01-05\n11.321981\n27\n53\n\n\n...\n...\n...\n...\n...\n...\n\n\n4869\nid_19\n2000-03-25\n40.060681\n97\n45\n\n\n4870\nid_19\n2000-03-26\n53.879482\n97\n45\n\n\n4871\nid_19\n2000-03-27\n62.020210\n97\n45\n\n\n4872\nid_19\n2000-03-28\n2.062543\n97\n45\n\n\n4873\nid_19\n2000-03-29\n14.151317\n97\n45\n\n\n\n\n4874 rows × 5 columns\n\n\n\nFor simplicity we’ll just take one time serie here.\n\nuids = series['unique_id'].unique()\nserie = series[series['unique_id'].eq(uids[0])]\nserie\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nstatic_1\n\n\n\n\n0\nid_00\n2000-01-01\n0.740453\n27\n53\n\n\n1\nid_00\n2000-01-02\n3.595262\n27\n53\n\n\n2\nid_00\n2000-01-03\n6.895835\n27\n53\n\n\n3\nid_00\n2000-01-04\n8.499450\n27\n53\n\n\n4\nid_00\n2000-01-05\n11.321981\n27\n53\n\n\n...\n...\n...\n...\n...\n...\n\n\n217\nid_00\n2000-08-05\n1.326319\n27\n53\n\n\n218\nid_00\n2000-08-06\n3.823198\n27\n53\n\n\n219\nid_00\n2000-08-07\n5.955518\n27\n53\n\n\n220\nid_00\n2000-08-08\n8.698637\n27\n53\n\n\n221\nid_00\n2000-08-09\n11.925481\n27\n53\n\n\n\n\n222 rows × 5 columns\n\n\n\n\n\nTimeSeries\n\n TimeSeries (freq:Union[int,str,pandas._libs.tslibs.offsets.BaseOffset,Non\n             eType]=None, lags:Optional[Iterable[int]]=None, lag_transform\n             s:Optional[Dict[int,List[Union[Callable,Tuple[Callable,Any]]]\n             ]]=None,\n             date_features:Optional[Iterable[Union[str,Callable]]]=None,\n             differences:Optional[Iterable[int]]=None, num_threads:int=1, \n             target_transforms:Optional[List[mlforecast.target_transforms.\n             BaseTargetTransform]]=None)\n\nUtility class for storing and transforming time series data.\nThe TimeSeries class takes care of defining the transformations to be performed (lags, lag_transforms and date_features). The transformations can be computed using multithreading if num_threads &gt; 1.\n\ndef month_start_or_end(dates):\n    return dates.is_month_start | dates.is_month_end\n\nflow_config = dict(\n    freq='W-THU',\n    lags=[7],\n    lag_transforms={\n        1: [expanding_mean, (rolling_mean, 7)]\n    },\n    date_features=['dayofweek', 'week', month_start_or_end]\n)\n\nts = TimeSeries(**flow_config)\nts\n\nTimeSeries(freq=&lt;Week: weekday=3&gt;, transforms=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag1_window_size7'], date_features=['dayofweek', 'week', 'month_start_or_end'], num_threads=1)\n\n\nThe frequency is converted to an offset.\n\ntest_eq(ts.freq, pd.tseries.frequencies.to_offset(flow_config['freq']))\n\nThe date features are stored as they were passed to the constructor.\n\ntest_eq(ts.date_features, flow_config['date_features'])\n\nThe transformations are stored as a dictionary where the key is the name of the transformation (name of the column in the dataframe with the computed features), which is built using build_transform_name and the value is a tuple where the first element is the lag it is applied to, then the function and then the function arguments.\n\ntest_eq(\n    ts.transforms, \n    {\n        'lag7': (7, _identity),\n        'expanding_mean_lag1': (1, expanding_mean), \n        'rolling_mean_lag1_window_size7': (1, rolling_mean, 7)\n        \n    }\n)\n\nNote that for lags we define the transformation as the identity function applied to its corresponding lag. This is because _transform_series takes the lag as an argument and shifts the array before computing the transformation."
  },
  {
    "objectID": "core.html#timeseries.fit_transform",
    "href": "core.html#timeseries.fit_transform",
    "title": "Core",
    "section": "TimeSeries.fit_transform",
    "text": "TimeSeries.fit_transform\n\n TimeSeries.fit_transform (data:pandas.core.frame.DataFrame, id_col:str,\n                           time_col:str, target_col:str,\n                           static_features:Optional[List[str]]=None,\n                           dropna:bool=True,\n                           keep_last_n:Optional[int]=None,\n                           max_horizon:Optional[int]=None,\n                           return_X_y:bool=False)\n\nAdd the features to data and save the required information for the predictions step.\nIf not all features are static, specify which ones are in static_features. If you don’t want to drop rows with null values after the transformations set dropna=False If keep_last_n is not None then that number of observations is kept across all series for updates.\n\nflow_config = dict(\n    freq='D',\n    lags=[7, 14],\n    lag_transforms={\n        2: [\n            (rolling_mean, 7),\n            (rolling_mean, 14),\n        ]\n    },\n    date_features=['dayofweek', 'month', 'year'],\n    num_threads=2\n)\n\nts = TimeSeries(**flow_config)\n_ = ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')\n\nThe series values are stored as a GroupedArray in an attribute ga. If the data type of the series values is an int then it is converted to np.float32, this is because lags generate np.nans so we need a float data type for them.\n\nnp.testing.assert_equal(ts.ga.data, series.y.values)\n\nThe series ids are stored in an uids attribute.\n\ntest_eq(ts.uids, series['unique_id'].unique())\n\nFor each time serie, the last observed date is stored so that predictions start from the last date + the frequency.\n\ntest_eq(ts.last_dates, series.groupby('unique_id')['ds'].max().values)\n\nThe last row of every serie without the y and ds columns are taken as static features.\n\npd.testing.assert_frame_equal(\n    ts.static_features_,\n    series.groupby('unique_id').tail(1).drop(columns=['ds', 'y']).reset_index(drop=True),\n)\n\nIf you pass static_features to TimeSeries.fit_transform then only these are kept.\n\nts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y', static_features=['static_0'])\n\npd.testing.assert_frame_equal(\n    ts.static_features_,\n    series.groupby('unique_id').tail(1)[['unique_id', 'static_0']].reset_index(drop=True),\n)\n\nYou can also specify keep_last_n in TimeSeries.fit_transform, which means that after computing the features for training we want to keep only the last n samples of each time serie for computing the updates. This saves both memory and time, since the updates are performed by running the transformation functions on all time series again and keeping only the last value (the update).\nIf you have very long time series and your updates only require a small sample it’s recommended that you set keep_last_n to the minimum number of samples required to compute the updates, which in this case is 15 since we have a rolling mean of size 14 over the lag 2 and in the first update the lag 2 becomes the lag 1. This is because in the first update the lag 1 is the last value of the series (or the lag 0), the lag 2 is the lag 1 and so on.\n\nkeep_last_n = 15\n\nts = TimeSeries(**flow_config)\ndf = ts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y', keep_last_n=keep_last_n)\nts._predict_setup()\n\nexpected_lags = ['lag7', 'lag14']\nexpected_transforms = ['rolling_mean_lag2_window_size7', \n                       'rolling_mean_lag2_window_size14']\nexpected_date_features = ['dayofweek', 'month', 'year']\n\ntest_eq(ts.features, expected_lags + expected_transforms + expected_date_features)\ntest_eq(ts.static_features_.columns.tolist() + ts.features, df.columns.drop(['ds', 'y']).tolist())\n# we dropped 2 rows because of the lag 2 and 13 more to have the window of size 14\ntest_eq(df.shape[0], series.shape[0] - (2 + 13) * ts.ga.ngroups)\ntest_eq(ts.ga.data.size, ts.ga.ngroups * keep_last_n)\n\nTimeSeries.fit_transform requires that the y column doesn’t have any null values. This is because the transformations could propagate them forward, so if you have null values in the y column you’ll get an error.\n\nseries_with_nulls = series.copy()\nseries_with_nulls.loc[1, 'y'] = np.nan\ntest_fail(\n    lambda: ts.fit_transform(series_with_nulls, id_col='unique_id', time_col='ds', target_col='y'),\n    contains='y column contains null values'\n)"
  },
  {
    "objectID": "core.html#timeseries.predict",
    "href": "core.html#timeseries.predict",
    "title": "Core",
    "section": "TimeSeries.predict",
    "text": "TimeSeries.predict\n\n TimeSeries.predict (models:Dict[str,Union[sklearn.base.BaseEstimator,List\n                     [sklearn.base.BaseEstimator]]], horizon:int, dynamic_\n                     dfs:Optional[List[pandas.core.frame.DataFrame]]=None,\n                     before_predict_callback:Optional[Callable]=None,\n                     after_predict_callback:Optional[Callable]=None)\n\nOnce we have a trained model we can use TimeSeries.predict passing the model and the horizon to get the predictions back.\n\nclass DummyModel:\n    def predict(self, X: pd.DataFrame) -&gt; np.ndarray:\n        return X['lag7'].values\n\nhorizon = 7\nmodel = DummyModel()\nts = TimeSeries(**flow_config)\nts.fit_transform(series, id_col='unique_id', time_col='ds', target_col='y')\npredictions = ts.predict({'DummyModel': model}, horizon)\n\ngrouped_series = series.groupby('unique_id')\nexpected_preds = grouped_series['y'].tail(7)  # the model predicts the lag-7\nlast_dates = grouped_series['ds'].max()\nexpected_dsmin = last_dates + ts.freq\nexpected_dsmax = last_dates + horizon * ts.freq\ngrouped_preds = predictions.groupby('unique_id')\n\nnp.testing.assert_allclose(predictions['DummyModel'], expected_preds)\npd.testing.assert_series_equal(grouped_preds['ds'].min(), expected_dsmin)\npd.testing.assert_series_equal(grouped_preds['ds'].max(), expected_dsmax)\n\nIf we have dynamic features we can pass them to dynamic_dfs.\n\nclass PredictPrice:\n    def predict(self, X):\n        return X['price']\n\nseries = generate_daily_series(20, n_static_features=2, equal_ends=True)\ndynamic_series = series.rename(columns={'static_1': 'product_id'})\nprices_catalog = generate_prices_for_series(dynamic_series)\nseries_with_prices = dynamic_series.merge(prices_catalog, how='left')\n\nmodel = PredictPrice()\nts = TimeSeries(**flow_config)\nts.fit_transform(\n    series_with_prices,\n    id_col='unique_id',\n    time_col='ds',\n    target_col='y',\n    static_features=['static_0', 'product_id'],\n)\npredictions = ts.predict({'PredictPrice': model}, horizon=1, dynamic_dfs=[prices_catalog])\n\nexpected_prices = series_with_prices.reset_index()[['unique_id', 'product_id']].drop_duplicates()\nexpected_prices['ds'] = series_with_prices['ds'].max() + ts.freq\nexpected_prices = expected_prices.reset_index()\nexpected_prices = expected_prices.merge(prices_catalog, on=['product_id', 'ds'], how='left')\nexpected_prices = expected_prices[['unique_id', 'ds', 'price']]\n\npd.testing.assert_frame_equal(\n    predictions.rename(columns={'PredictPrice': 'price'}),\n    expected_prices\n)"
  },
  {
    "objectID": "core.html#timeseries.update",
    "href": "core.html#timeseries.update",
    "title": "Core",
    "section": "TimeSeries.update",
    "text": "TimeSeries.update\n\n TimeSeries.update (df:pandas.core.frame.DataFrame)\n\nUpdate the values of the stored series."
  },
  {
    "objectID": "target_transforms.html",
    "href": "target_transforms.html",
    "title": "mlforecast",
    "section": "",
    "text": "BaseTargetTransform\n\n BaseTargetTransform ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\nDifferences\n\n Differences (differences:Iterable[int])\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\nLocalStandardScaler\n\n LocalStandardScaler ()\n\nStandardizes each serie by subtracting its mean and dividing by its standard deviation.\n\nimport pandas as pd\n\nfrom mlforecast.utils import generate_daily_series\n\nseries = generate_daily_series(10, min_length=50, max_length=50)\nsc = LocalStandardScaler()\nsc.set_column_names('unique_id', 'ds', 'y')\npd.testing.assert_frame_equal(\n    sc.inverse_transform(sc.fit_transform(series)),\n    series,\n)\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "from fastcore.test import test_eq, test_fail, test_warns\n\n\n\ngenerate_daily_series\n\n generate_daily_series (n_series:int, min_length:int=50,\n                        max_length:int=500, n_static_features:int=0,\n                        equal_ends:bool=False,\n                        static_as_categorical:bool=True,\n                        with_trend:bool=False, seed:int=0)\n\nGenerates n_series of different lengths in the interval [min_length, max_length].\nIf n_static_features &gt; 0, then each serie gets static features with random values. If equal_ends == True then all series end at the same date.\nGenerate 20 series with lengths between 100 and 1,000.\n\nn_series = 20\nmin_length = 100\nmax_length = 1000\n\nseries = generate_daily_series(n_series, min_length, max_length)\nseries\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nid_00\n2000-01-01\n0.395863\n\n\n1\nid_00\n2000-01-02\n1.264447\n\n\n2\nid_00\n2000-01-03\n2.284022\n\n\n3\nid_00\n2000-01-04\n3.462798\n\n\n4\nid_00\n2000-01-05\n4.035518\n\n\n...\n...\n...\n...\n\n\n12446\nid_19\n2002-03-11\n0.309275\n\n\n12447\nid_19\n2002-03-12\n1.189464\n\n\n12448\nid_19\n2002-03-13\n2.325032\n\n\n12449\nid_19\n2002-03-14\n3.333198\n\n\n12450\nid_19\n2002-03-15\n4.306117\n\n\n\n\n12451 rows × 3 columns\n\n\n\n\nseries_sizes = series.groupby('unique_id').size()\nassert series_sizes.size == n_series\nassert series_sizes.min() &gt;= min_length\nassert series_sizes.max() &lt;= max_length\n\nWe can also add static features to each serie (these can be things like product_id or store_id). Only the first static feature (static_0) is relevant to the target.\n\nn_static_features = 2\n\nseries_with_statics = generate_daily_series(n_series, min_length, max_length, n_static_features)\nseries_with_statics\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nstatic_1\n\n\n\n\n0\nid_00\n2000-01-01\n0.752139\n18\n10\n\n\n1\nid_00\n2000-01-02\n2.402450\n18\n10\n\n\n2\nid_00\n2000-01-03\n4.339642\n18\n10\n\n\n3\nid_00\n2000-01-04\n6.579317\n18\n10\n\n\n4\nid_00\n2000-01-05\n7.667484\n18\n10\n\n\n...\n...\n...\n...\n...\n...\n\n\n12446\nid_19\n2002-03-11\n2.783477\n89\n42\n\n\n12447\nid_19\n2002-03-12\n10.705175\n89\n42\n\n\n12448\nid_19\n2002-03-13\n20.925285\n89\n42\n\n\n12449\nid_19\n2002-03-14\n29.998780\n89\n42\n\n\n12450\nid_19\n2002-03-15\n38.755054\n89\n42\n\n\n\n\n12451 rows × 5 columns\n\n\n\n\nfor i in range(n_static_features):\n    assert all(series_with_statics.groupby('unique_id')[f'static_{i}'].nunique() == 1)\n\nIf equal_ends=False (the default) then every serie has a different end date.\n\nassert series_with_statics.groupby('unique_id')['ds'].max().nunique() &gt; 1\n\nWe can have all of them end at the same date by specifying equal_ends=True.\n\nseries_equal_ends = generate_daily_series(n_series, min_length, max_length, equal_ends=True)\n\nassert series_equal_ends.groupby('unique_id')['ds'].max().nunique() == 1\n\n\n\n\ngenerate_prices_for_series\n\n generate_prices_for_series (series:pandas.core.frame.DataFrame,\n                             horizon:int=7, seed:int=0)\n\n\nseries_for_prices = generate_daily_series(20, n_static_features=2, equal_ends=True)\nseries_for_prices.rename(columns={'static_1': 'product_id'}, inplace=True)\nprices_catalog = generate_prices_for_series(series_for_prices, horizon=7)\nprices_catalog\n\n\n\n\n\n\n\n\nds\nproduct_id\nprice\n\n\n\n\n0\n2000-05-07\n9\n0.548814\n\n\n1\n2000-05-08\n9\n0.715189\n\n\n2\n2000-05-09\n9\n0.602763\n\n\n3\n2000-05-10\n9\n0.544883\n\n\n4\n2000-05-11\n9\n0.423655\n\n\n...\n...\n...\n...\n\n\n4263\n2001-05-17\n93\n0.800781\n\n\n4264\n2001-05-18\n93\n0.909013\n\n\n4265\n2001-05-19\n93\n0.904419\n\n\n4266\n2001-05-20\n93\n0.327888\n\n\n4267\n2001-05-21\n93\n0.971973\n\n\n\n\n4268 rows × 3 columns\n\n\n\n\ntest_eq(set(prices_catalog['product_id']), set(series_for_prices['product_id']))\ntest_fail(lambda: generate_prices_for_series(series_equal_ends), contains='product_id')\ntest_fail(lambda: generate_prices_for_series(series), contains='equal ends')\n\n\n\n\nbacktest_splits\n\n backtest_splits (df:pandas.core.frame.DataFrame, n_windows:int, h:int,\n                  id_col:str, time_col:str,\n                  freq:Union[pandas._libs.tslibs.offsets.BaseOffset,int],\n                  step_size:Optional[int]=None,\n                  input_size:Optional[int]=None)\n\n\n\n\nPredictionIntervals\n\n PredictionIntervals (n_windows:int=2, h:int=1,\n                      method:str='conformal_distribution',\n                      window_size:Optional[int]=None)\n\nClass for storing prediction intervals metadata information.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_windows\nint\n2\n\n\n\nh\nint\n1\n\n\n\nmethod\nstr\nconformal_distribution\n\n\n\nwindow_size\ntyping.Optional[int]\nNone\nnoqa: ARG002\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.models.spark.xgb.html",
    "href": "distributed.models.spark.xgb.html",
    "title": "SparkXGBForecast",
    "section": "",
    "text": "Wrapper of xgboost.spark.SparkXGBRegressor that adds an extract_local_model method to get a local version of the trained model and broadcast it to the workers.\n/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n\n\nSparkXGBForecast\n\n SparkXGBForecast (**kwargs)\n\nSparkXGBRegressor is a PySpark ML estimator. It implements the XGBoost regression algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like :py:class:~pyspark.ml.tuning.CrossValidator/ :py:class:~pyspark.ml.tuning.TrainValidationSplit/ :py:class:~pyspark.ml.classification.OneVsRest\nSparkXGBRegressor automatically supports most of the parameters in xgboost.XGBRegressor constructor and most of the parameters used in :py:class:xgboost.XGBRegressor fit and predict method.\nSparkXGBRegressor doesn’t support setting gpu_id but support another param use_gpu, see doc below for more details.\nSparkXGBRegressor doesn’t support setting base_margin explicitly as well, but support another param called base_margin_col. see doc below for more details.\nSparkXGBRegressor doesn’t support validate_features and output_margin param.\nSparkXGBRegressor doesn’t support setting nthread xgboost param, instead, the nthread param for each xgboost worker will be set equal to spark.task.cpus config value.\ncallbacks: The export and import of the callback functions are at best effort. For details, see :py:attr:xgboost.spark.SparkXGBRegressor.callbacks param doc. validation_indicator_col For params related to xgboost.XGBRegressor training with evaluation dataset’s supervision, set :py:attr:xgboost.spark.SparkXGBRegressor.validation_indicator_col parameter instead of setting the eval_set parameter in xgboost.XGBRegressor fit method. weight_col: To specify the weight of the training and validation dataset, set :py:attr:xgboost.spark.SparkXGBRegressor.weight_col parameter instead of setting sample_weight and sample_weight_eval_set parameter in xgboost.XGBRegressor fit method. xgb_model: Set the value to be the instance returned by :func:xgboost.spark.SparkXGBRegressorModel.get_booster. num_workers: Integer that specifies the number of XGBoost workers to use. Each XGBoost worker corresponds to one spark task. use_gpu: Boolean that specifies whether the executors are running on GPU instances. base_margin_col: To specify the base margins of the training and validation dataset, set :py:attr:xgboost.spark.SparkXGBRegressor.base_margin_col parameter instead of setting base_margin and base_margin_eval_set in the xgboost.XGBRegressor fit method. Note: this isn’t available for distributed training.\n.. Note:: The Parameters chart above contains parameters that need special handling. For a full list of parameters, see entries with Param(parent=... below.\n.. Note:: This API is experimental.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.models.dask.lgb.html",
    "href": "distributed.models.dask.lgb.html",
    "title": "DaskLGBMForecast",
    "section": "",
    "text": "Wrapper of lightgbm.dask.DaskLGBMRegressor that adds a model_ property that contains the fitted booster and is sent to the workers to in the forecasting step.\n\n\nDaskLGBMForecast\n\n DaskLGBMForecast (boosting_type:str='gbdt', num_leaves:int=31,\n                   max_depth:int=-1, learning_rate:float=0.1,\n                   n_estimators:int=100, subsample_for_bin:int=200000, obj\n                   ective:Union[str,Callable[[Optional[numpy.ndarray],nump\n                   y.ndarray],Tuple[numpy.ndarray,numpy.ndarray]],Callable\n                   [[Optional[numpy.ndarray],numpy.ndarray,Optional[numpy.\n                   ndarray]],Tuple[numpy.ndarray,numpy.ndarray]],Callable[\n                   [Optional[numpy.ndarray],numpy.ndarray,Optional[numpy.n\n                   darray],Optional[numpy.ndarray]],Tuple[numpy.ndarray,nu\n                   mpy.ndarray]],NoneType]=None,\n                   class_weight:Union[dict,str,NoneType]=None,\n                   min_split_gain:float=0.0, min_child_weight:float=0.001,\n                   min_child_samples:int=20, subsample:float=1.0,\n                   subsample_freq:int=0, colsample_bytree:float=1.0,\n                   reg_alpha:float=0.0, reg_lambda:float=0.0, random_state\n                   :Union[int,numpy.random.mtrand.RandomState,NoneType]=No\n                   ne, n_jobs:Optional[int]=None,\n                   importance_type:str='split',\n                   client:Optional[distributed.client.Client]=None,\n                   **kwargs:Any)\n\nDistributed version of lightgbm.LGBMRegressor.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "MLForecast",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "forecast.html#example",
    "href": "forecast.html#example",
    "title": "MLForecast",
    "section": "Example",
    "text": "Example\nThis shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to this notebook.\n\nimport random\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xgboost as xgb\nfrom datasetsforecast.m4 import M4, M4Info\nfrom sklearn.metrics import mean_squared_error\nfrom window_ops.ewm import ewm_mean\nfrom window_ops.expanding import expanding_mean\nfrom window_ops.rolling import rolling_mean\n\nfrom mlforecast.lgb_cv import LightGBMCV\nfrom mlforecast.target_transforms import Differences\nfrom mlforecast.utils import generate_daily_series, generate_prices_for_series\n\n\ngroup = 'Hourly'\nawait M4.async_download('data', group=group)\ndf, *_ = M4.load(directory='data', group=group)\ndf['ds'] = df['ds'].astype('int')\nids = df['unique_id'].unique()\nrandom.seed(0)\nsample_ids = random.choices(ids, k=4)\nsample_df = df[df['unique_id'].isin(sample_ids)]\nsample_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n86796\nH196\n1\n11.8\n\n\n86797\nH196\n2\n11.4\n\n\n86798\nH196\n3\n11.1\n\n\n86799\nH196\n4\n10.8\n\n\n86800\nH196\n5\n10.6\n\n\n...\n...\n...\n...\n\n\n325235\nH413\n1004\n99.0\n\n\n325236\nH413\n1005\n88.0\n\n\n325237\nH413\n1006\n47.0\n\n\n325238\nH413\n1007\n41.0\n\n\n325239\nH413\n1008\n34.0\n\n\n\n\n4032 rows × 3 columns\n\n\n\nWe now split this data into train and validation.\n\ninfo = M4Info[group]\nhorizon = info.horizon\nvalid = sample_df.groupby('unique_id').tail(horizon)\ntrain = sample_df.drop(valid.index)\ntrain.shape, valid.shape\n\n((3840, 3), (192, 3))\n\n\n\nCreating the Forecast object\nThe forecast object encapsulates the feature engineering + training the models + forecasting. When we initialize it we define:\n\nThe models we want to train\nThe series frequency. This is added to the last dates seen in train for the forecast step, if the time column contains integer values we can leave it empty or set it to 1.\nThe feature engineering:\n\nLags to use as features\nTransformations on the lags\nDate features\nDifferences to apply to the target before computing the features, which are then restored when forecasting.\n\nNumber of threads to use when computing the features.\n\n\nfcst = MLForecast(\n    models=lgb.LGBMRegressor(random_state=0),\n    lags=[24 * (i+1) for i in range(7)],\n    lag_transforms={\n        48: [(ewm_mean, 0.3)],\n    },\n    num_threads=1,\n    target_transforms=[Differences([24])],\n)\nfcst\n\nMLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)\n\n\nOnce we have this setup we can compute the features and fit the model.\n\n\n\nMLForecast.fit\n\n MLForecast.fit (df:pandas.core.frame.DataFrame, id_col:str='unique_id',\n                 time_col:str='ds', target_col:str='y',\n                 static_features:Optional[List[str]]=None,\n                 dropna:bool=True, keep_last_n:Optional[int]=None,\n                 max_horizon:Optional[int]=None, prediction_intervals:Opti\n                 onal[mlforecast.utils.PredictionIntervals]=None,\n                 data:Optional[pandas.core.frame.DataFrame]=None)\n\nApply the feature engineering and train the models.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nSeries data in long format.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\nmax_horizon\ntyping.Optional[int]\nNone\n\n\n\nprediction_intervals\ntyping.Optional[mlforecast.utils.PredictionIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\ndata\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data in long format. This argument has been replaced by df and will be removed in a later release.\n\n\nReturns\nMLForecast\n\nnoqa: ARG002\n\n\n\n\nfcst.fit(train);\n\nOnce we’ve run this we’re ready to compute our predictions.\n\n\n\nMLForecast.predict\n\n MLForecast.predict (h:int,\n                     dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n                     ]]=None,\n                     before_predict_callback:Optional[Callable]=None,\n                     after_predict_callback:Optional[Callable]=None,\n                     new_df:Optional[pandas.core.frame.DataFrame]=None,\n                     level:Optional[List[Union[int,float]]]=None,\n                     horizon:Optional[int]=None,\n                     new_data:Optional[pandas.core.frame.DataFrame]=None)\n\nCompute the predictions for the next horizon steps.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nNumber of periods to predict.\n\n\ndynamic_dfs\ntyping.Optional[typing.List[pandas.core.frame.DataFrame]]\nNone\nFuture values of the dynamic features, e.g. prices.\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\nnew_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data of new observations for which forecasts are to be generated.  This dataframe should have the same structure as the one used to fit the model, including any features and time series data.  If new_df is not None, the method will generate forecasts for the new observations.\n\n\nlevel\ntyping.Optional[typing.List[typing.Union[int, float]]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nhorizon\ntyping.Optional[int]\nNone\nNumber of periods to predict. This argument has been replaced by h and will be removed in a later release.\n\n\nnew_data\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data of new observations for which forecasts are to be generated.  This dataframe should have the same structure as the one used to fit the model, including any features and time series data.  If new_data is not None, the method will generate forecasts for the new observations.\n\n\nReturns\nDataFrame\n\nnoqa: ARG002noqa: ARG002 \n\n\n\n\npredictions = fcst.predict(horizon)\n\nWe can see at a couple of results.\n\nresults = valid.merge(predictions, on=['unique_id', 'ds']).set_index('unique_id')\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\nfor uid, axi in zip(sample_ids, ax.flat):\n    results.loc[uid].set_index('ds').plot(ax=axi, title=uid)\nfig.savefig('figs/forecast__predict.png', bbox_inches='tight')\nplt.close()\n\n\n\nPrediction intervals\nWith MLForecast, you can generate prediction intervals using Conformal Prediction. To configure Conformal Prediction, you need to pass an instance of the PredictionIntervals class to the prediction_intervals argument of the fit method. The class takes three parameters: n_windows, h and method.\n\nn_windows represents the number of cross-validation windows used to calibrate the intervals\nh is the forecast horizon\nmethod can be conformal_distribution or conformal_error; conformal_distribution (default) creates forecasts paths based on the cross-validation errors and calculate quantiles using those paths, on the other hand conformal_error calculates the error quantiles to produce prediction intervals. The strategy will adjust the intervals for each horizon step, resulting in different widths for each step. Please note that a minimum of 2 cross-validation windows must be used.\n\n\nfcst.fit(\n    train, \n    prediction_intervals=PredictionIntervals(n_windows=3, h=48)\n);\n\nAfter that, you just have to include your desired confidence levels to the predict method using the level argument. Levels must lie between 0 and 100.\n\npredictions_w_intervals = fcst.predict(48, level=[50, 80, 95])\n\n\npredictions_w_intervals.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\nLGBMRegressor-lo-95\nLGBMRegressor-lo-80\nLGBMRegressor-lo-50\nLGBMRegressor-hi-50\nLGBMRegressor-hi-80\nLGBMRegressor-hi-95\n\n\n\n\n0\nH196\n961\n16.071271\n15.958042\n15.971271\n16.005091\n16.137452\n16.171271\n16.184501\n\n\n1\nH196\n962\n15.671271\n15.553632\n15.553632\n15.578632\n15.763911\n15.788911\n15.788911\n\n\n2\nH196\n963\n15.271271\n15.153632\n15.153632\n15.162452\n15.380091\n15.388911\n15.388911\n\n\n3\nH196\n964\n14.971271\n14.858042\n14.871271\n14.905091\n15.037452\n15.071271\n15.084501\n\n\n4\nH196\n965\n14.671271\n14.553632\n14.553632\n14.562452\n14.780091\n14.788911\n14.788911\n\n\n\n\n\n\n\n\n# test we can forecast horizon lower than h \n# with prediction intervals\nfor method in ['conformal_distribution', 'conformal_errors']:\n    fcst.fit(\n        train, \n        prediction_intervals=PredictionIntervals(n_windows=3, h=48)\n    )\n\n    preds_h_lower_h = fcst.predict(1, level=[50, 80, 95])\n    preds_h_lower_h = fcst.predict(30, level=[50, 80, 95])\n\n    # test monotonicity of intervals\n    test_eq(\n        preds_h_lower_h.filter(regex='lo|hi').apply(\n            lambda x: x.is_monotonic_increasing,\n            axis=1\n        ).sum(),\n        len(preds_h_lower_h)\n    )\n\nLet’s explore the generated intervals.\n\nresults = valid.merge(predictions_w_intervals, on=['unique_id', 'ds']).set_index('unique_id')\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\nfor uid, axi in zip(sample_ids, ax.flat):\n    uid_results = results.loc[uid].set_index('ds')\n    uid_results[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n    for lv in [50, 80, 95]:\n        axi.fill_between(\n            uid_results.index, \n            uid_results[f'LGBMRegressor-lo-{lv}'].values, \n            uid_results[f'LGBMRegressor-hi-{lv}'].values,\n            label=f'LGBMRegressor-level-{lv}',\n            color='orange',\n            alpha=1 - lv / 100\n        )\n    axi.legend()\nfig.savefig('figs/forecast__predict_intervals.png', bbox_inches='tight')\nplt.close()\n\n\nIf you want to reduce the computational time and produce intervals with the same width for the whole forecast horizon, simple pass h=1 to the PredictionIntervals class. The caveat of this strategy is that in some cases, variance of the absolute residuals maybe be small (even zero), so the intervals may be too narrow.\n\nfcst.fit(\n    train,  \n    prediction_intervals=PredictionIntervals(n_windows=3, h=1)\n);\n\n\npredictions_w_intervals_ws_1 = fcst.predict(48, level=[80, 90, 95])\n\nLet’s explore the generated intervals.\n\nresults = valid.merge(predictions_w_intervals_ws_1, on=['unique_id', 'ds']).set_index('unique_id')\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\nfor uid, axi in zip(sample_ids, ax.flat):\n    uid_results = results.loc[uid].set_index('ds')\n    uid_results[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n    axi.fill_between(\n        uid_results.index, \n        uid_results['LGBMRegressor-lo-90'].values, \n        uid_results['LGBMRegressor-hi-90'].values,\n        label='LGBMRegressor-level-90',\n        color='orange',\n        alpha=0.2\n    )\n    axi.legend()\nfig.savefig('figs/forecast__predict_intervals_window_size_1.png', bbox_inches='tight')\nplt.close()\n\n\n\n\nForecast using a pretrained model\nMLForecast allows you to use a pretrained model to generate forecasts for a new dataset. Simply provide a pandas dataframe containing the new observations as the value for the new_data argument when calling the predict method. The dataframe should have the same structure as the one used to fit the model, including any features and time series data. The function will then use the pretrained model to generate forecasts for the new observations. This allows you to easily apply a pretrained model to a new dataset and generate forecasts without the need to retrain the model.\n\nercot_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\n# we have to convert the ds column to integers\n# since MLForecast was trained with that structure\nercot_df['ds'] = np.arange(1, len(ercot_df) + 1)\n# use the `new_data` argument to pass the ercot dataset \nercot_fcsts = fcst.predict(horizon, new_df=ercot_df)\nfig, ax = plt.subplots()\nercot_df.tail(48 * 2).plot(x='ds', y='y', figsize=(20, 7), ax=ax)\nercot_fcsts.plot(x='ds', y='LGBMRegressor', ax=ax, title='ERCOT forecasts trained on M4-Hourly dataset');\nplt.gcf().savefig('figs/forecast__ercot.png', bbox_inches='tight')\nplt.close()\n\n\nIf you want to take a look at the data that will be used to train the models you can call Forecast.preprocess.\n\n\n\n\nMLForecast.preprocess\n\n MLForecast.preprocess (df:pandas.core.frame.DataFrame,\n                        id_col:str='unique_id', time_col:str='ds',\n                        target_col:str='y',\n                        static_features:Optional[List[str]]=None,\n                        dropna:bool=True, keep_last_n:Optional[int]=None,\n                        max_horizon:Optional[int]=None,\n                        return_X_y:bool=False,\n                        data:Optional[pandas.core.frame.DataFrame]=None)\n\nAdd the features to data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nSeries data in long format.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\nmax_horizon\ntyping.Optional[int]\nNone\n\n\n\nreturn_X_y\nbool\nFalse\n\n\n\ndata\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data in long format. This argument has been replaced by df and will be removed in a later release.\n\n\nReturns\ntyping.Union[pandas.core.frame.DataFrame, typing.Tuple[pandas.core.frame.DataFrame, typing.Union[pandas.core.series.Series, pandas.core.frame.DataFrame]]]\n\nnoqa: ARG002\n\n\n\n\nprep_df = fcst.preprocess(train)\nprep_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nlag24\nlag48\nlag72\nlag96\nlag120\nlag144\nlag168\newm_mean_lag48_alpha0.3\n\n\n\n\n86988\nH196\n193\n0.1\n0.0\n0.0\n0.0\n0.3\n0.1\n0.1\n0.3\n0.002810\n\n\n86989\nH196\n194\n0.1\n-0.1\n0.1\n0.0\n0.3\n0.1\n0.1\n0.3\n0.031967\n\n\n86990\nH196\n195\n0.1\n-0.1\n0.1\n0.0\n0.3\n0.1\n0.2\n0.1\n0.052377\n\n\n86991\nH196\n196\n0.1\n0.0\n0.0\n0.0\n0.3\n0.2\n0.1\n0.2\n0.036664\n\n\n86992\nH196\n197\n0.0\n0.0\n0.0\n0.1\n0.2\n0.2\n0.1\n0.2\n0.025665\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n325187\nH413\n956\n0.0\n10.0\n1.0\n6.0\n-53.0\n44.0\n-21.0\n21.0\n7.963225\n\n\n325188\nH413\n957\n9.0\n10.0\n10.0\n-7.0\n-46.0\n27.0\n-19.0\n24.0\n8.574257\n\n\n325189\nH413\n958\n16.0\n8.0\n5.0\n-9.0\n-36.0\n32.0\n-13.0\n8.0\n7.501980\n\n\n325190\nH413\n959\n-3.0\n17.0\n-7.0\n2.0\n-31.0\n22.0\n5.0\n-2.0\n3.151386\n\n\n325191\nH413\n960\n15.0\n11.0\n-6.0\n-5.0\n-17.0\n22.0\n-18.0\n10.0\n0.405970\n\n\n\n\n3072 rows × 11 columns\n\n\n\nIf we do this we then have to call Forecast.fit_models, since this only stores the series information.\n\n\n\nMLForecast.fit_models\n\n MLForecast.fit_models (X:pandas.core.frame.DataFrame,\n                        y:Union[pandas.core.series.Series,pandas.core.fram\n                        e.DataFrame])\n\nManually train models. Use this if you called Forecast.preprocess beforehand.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nDataFrame\nFeatures.\n\n\ny\ntyping.Union[pandas.core.series.Series, pandas.core.frame.DataFrame]\nTarget.\n\n\nReturns\nMLForecast\nForecast object with trained models.\n\n\n\n\nX, y = prep_df.drop(columns=['unique_id', 'ds', 'y']), prep_df['y']\nfcst.fit_models(X, y)\n\nMLForecast(models=[LGBMRegressor], freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168', 'ewm_mean_lag48_alpha0.3'], date_features=[], num_threads=1)\n\n\n\npredictions2 = fcst.predict(horizon)\npd.testing.assert_frame_equal(predictions, predictions2)\n\n\n\nMulti-output model\nBy default mlforecast uses the recursive strategy, i.e. a model is trained to predict the next value and if we’re predicting several values we do it one at a time and then use the model’s predictions as the new target, recompute the features and predict the next step.\nThere’s another approach where if we want to predict 10 steps ahead we train 10 different models, where each model is trained to predict the value at each specific step, i.e. one model predicts the next value, another one predicts the value two steps ahead and so on. This can be very time consuming but can also provide better results. If you want to use this approach you can specify max_horizon in MLForecast.fit, which will train that many models and each model will predict its corresponding horizon when you call MLForecast.predict.\n\ndef avg_mape(df):\n    full = df.merge(valid)\n    return abs(full['LGBMRegressor'] - full['y']).div(full['y']).groupby(full['unique_id']).mean()\n\n\nfcst = MLForecast(\n    models=lgb.LGBMRegressor(random_state=0),\n    lags=[24 * (i+1) for i in range(7)],\n    lag_transforms={\n        1: [(rolling_mean, 24)],\n        24: [(rolling_mean, 24)],\n        48: [(ewm_mean, 0.3)],\n    },\n    num_threads=1,\n    target_transforms=[Differences([24])],\n)\n\n\nmax_horizon = 24\n# the following will train 24 models, one for each horizon\nindividual_fcst = fcst.fit(train, max_horizon=max_horizon)\nindividual_preds = individual_fcst.predict(max_horizon)\navg_mape_individual = avg_mape(individual_preds).rename('individual')\n# the following will train a single model and use the recursive strategy\nrecursive_fcst = fcst.fit(train)\nrecursive_preds = recursive_fcst.predict(max_horizon)\navg_mape_recursive = avg_mape(recursive_preds).rename('recursive')\n# results\nprint('Average MAPE per method and serie')\navg_mape_individual.to_frame().join(avg_mape_recursive).applymap('{:.1%}'.format)\n\nAverage MAPE per method and serie\n\n\n\n\n\n\n\n\n\nindividual\nrecursive\n\n\nunique_id\n\n\n\n\n\n\nH196\n0.5%\n0.6%\n\n\nH256\n0.7%\n0.6%\n\n\nH381\n48.9%\n20.3%\n\n\nH413\n26.9%\n35.1%\n\n\n\n\n\n\n\n\n\nCross validation\nIf we would like to know how good our forecast will be for a specific model and set of features then we can perform cross validation. What cross validation does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last x observations from our data as the validation set.\nThis process is implemented in MLForecast.cross_validation, which takes our data and performs the process described above for n_windows times where each window has h validation samples in it. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n\nTrain: 1 to 72. Validation: 73 to 86.\nTrain: 1 to 86. Validation: 87 to 100.\n\nYou can control the size between each cross validation window using the step_size argument. For example, if we have 100 samples and we want to perform 2 backtests each of size 14 and move one step ahead in each fold (step_size=1), the splits will be as follows:\n\nTrain: 1 to 85. Validation: 86 to 99.\nTrain: 1 to 86. Validation: 87 to 100.\n\nYou can also perform cross validation without refitting your models for each window by setting refit=False. This allows you to evaluate the performance of your models using multiple window sizes without having to retrain them each time.\n\n\n\nMLForecast.cross_validation\n\n MLForecast.cross_validation (df:pandas.core.frame.DataFrame,\n                              n_windows:int, h:int,\n                              id_col:str='unique_id', time_col:str='ds',\n                              target_col:str='y',\n                              step_size:Optional[int]=None,\n                              static_features:Optional[List[str]]=None,\n                              dropna:bool=True,\n                              keep_last_n:Optional[int]=None,\n                              refit:bool=True,\n                              max_horizon:Optional[int]=None, before_predi\n                              ct_callback:Optional[Callable]=None, after_p\n                              redict_callback:Optional[Callable]=None, pre\n                              diction_intervals:Optional[mlforecast.utils.\n                              PredictionIntervals]=None,\n                              level:Optional[List[Union[int,float]]]=None,\n                              input_size:Optional[int]=None,\n                              fitted:bool=False, data:Optional[pandas.core\n                              .frame.DataFrame]=None,\n                              window_size:Optional[int]=None)\n\nPerform time series cross validation. Creates n_windows splits where each window has h test periods, trains the models, computes the predictions and merges the actuals.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nSeries data in long format.\n\n\nn_windows\nint\n\nNumber of windows to evaluate.\n\n\nh\nint\n\nForecast horizon.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstep_size\ntyping.Optional[int]\nNone\nStep size between each cross validation window. If None it will be equal to h.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\nrefit\nbool\nTrue\nRetrain model for each cross validation window.If False, the models are trained at the beginning and then used to predict each window.\n\n\nmax_horizon\ntyping.Optional[int]\nNone\n\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\nprediction_intervals\ntyping.Optional[mlforecast.utils.PredictionIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nlevel\ntyping.Optional[typing.List[typing.Union[int, float]]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\ninput_size\ntyping.Optional[int]\nNone\nMaximum training samples per serie in each window. If None, will use an expanding window.\n\n\nfitted\nbool\nFalse\nStore the in-sample predictions.\n\n\ndata\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data in long format. This argument has been replaced by df and will be removed in a later release.\n\n\nwindow_size\ntyping.Optional[int]\nNone\nForecast horizon. This argument has been replaced by h and will be removed in a later release.\n\n\nReturns\npandas DataFrame\n\nPredictions for each window with the series id, timestamp, last train date, target value and predictions from each model.\n\n\n\n\nfcst = MLForecast(\n    models=lgb.LGBMRegressor(random_state=0),\n    lags=[24 * (i+1) for i in range(7)],\n    lag_transforms={\n        1: [(rolling_mean, 24)],\n        24: [(rolling_mean, 24)],\n        48: [(ewm_mean, 0.3)],\n    },\n    num_threads=1,\n    target_transforms=[Differences([24])],\n)\ncv_results = fcst.cross_validation(\n    train,\n    n_windows=4,\n    h=horizon,\n    step_size=horizon,\n    fitted=True,\n)\ncv_results\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\n\n\n\n\n0\nH196\n769\n768\n15.2\n15.167163\n\n\n1\nH196\n770\n768\n14.8\n14.767163\n\n\n2\nH196\n771\n768\n14.4\n14.467163\n\n\n3\nH196\n772\n768\n14.1\n14.167163\n\n\n4\nH196\n773\n768\n13.8\n13.867163\n\n\n...\n...\n...\n...\n...\n...\n\n\n187\nH413\n956\n912\n59.0\n64.284167\n\n\n188\nH413\n957\n912\n58.0\n64.830429\n\n\n189\nH413\n958\n912\n53.0\n40.726851\n\n\n190\nH413\n959\n912\n38.0\n42.739657\n\n\n191\nH413\n960\n912\n46.0\n52.802769\n\n\n\n\n768 rows × 5 columns\n\n\n\nSince we set fitted=True we can access the predictions for the training sets as well with the cross_validation_fitted_values method.\n\nfcst.cross_validation_fitted_values()\n\n\n\n\n\n\n\n\nunique_id\nds\nfold\ny\nLGBMRegressor\n\n\n\n\n0\nH196\n1\n0\n11.8\n15.167163\n\n\n1\nH196\n2\n0\n11.4\n14.767163\n\n\n2\nH196\n3\n0\n11.1\n14.467163\n\n\n3\nH196\n4\n0\n10.8\n14.167163\n\n\n4\nH196\n5\n0\n10.6\n13.867163\n\n\n...\n...\n...\n...\n...\n...\n\n\n13435\nH413\n908\n3\n49.0\n40.262691\n\n\n13436\nH413\n909\n3\n39.0\n26.603123\n\n\n13437\nH413\n910\n3\n29.0\n42.545732\n\n\n13438\nH413\n911\n3\n24.0\n30.053714\n\n\n13439\nH413\n912\n3\n20.0\n-13.589900\n\n\n\n\n13440 rows × 5 columns\n\n\n\nWe can also compute prediction intervals by passing a configuration to prediction_intervals as well as values for the width through levels.\n\ncv_results_intervals = fcst.cross_validation(\n    train,\n    n_windows=4,\n    h=horizon,\n    step_size=horizon,\n    prediction_intervals=PredictionIntervals(h=horizon),\n    level=[80, 90]\n)\ncv_results_intervals\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\nLGBMRegressor-lo-90\nLGBMRegressor-lo-80\nLGBMRegressor-hi-80\nLGBMRegressor-hi-90\n\n\n\n\n0\nH196\n769\n768\n15.2\n15.167163\n15.141751\n15.141751\n15.192575\n15.192575\n\n\n1\nH196\n770\n768\n14.8\n14.767163\n14.741751\n14.741751\n14.792575\n14.792575\n\n\n2\nH196\n771\n768\n14.4\n14.467163\n14.399951\n14.407328\n14.526998\n14.534374\n\n\n3\nH196\n772\n768\n14.1\n14.167163\n14.092575\n14.092575\n14.241751\n14.241751\n\n\n4\nH196\n773\n768\n13.8\n13.867163\n13.792575\n13.792575\n13.941751\n13.941751\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n187\nH413\n956\n912\n59.0\n64.284167\n29.890099\n34.371545\n94.196788\n98.678234\n\n\n188\nH413\n957\n912\n58.0\n64.830429\n56.874572\n57.827689\n71.833169\n72.786285\n\n\n189\nH413\n958\n912\n53.0\n40.726851\n35.296195\n35.846206\n45.607495\n46.157506\n\n\n190\nH413\n959\n912\n38.0\n42.739657\n35.292153\n35.807640\n49.671674\n50.187161\n\n\n191\nH413\n960\n912\n46.0\n52.802769\n42.465597\n43.895670\n61.709869\n63.139941\n\n\n\n\n768 rows × 9 columns\n\n\n\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n\nfor uid, axi in zip(sample_ids, ax.flat):\n    subset = cv_results[cv_results['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff'])\n    subset.set_index('ds').plot(ax=axi, title=uid)\nfig.savefig('figs/forecast__cross_validation.png', bbox_inches='tight')\nplt.close()\n\n\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n\nfor uid, axi in zip(sample_ids, ax.flat):\n    subset = cv_results_intervals[cv_results_intervals['unique_id'].eq(uid)].drop(columns=['unique_id', 'cutoff']).set_index('ds')\n    subset[['y', 'LGBMRegressor']].plot(ax=axi, title=uid)\n    axi.fill_between(\n        subset.index, \n        subset['LGBMRegressor-lo-90'].values, \n        subset['LGBMRegressor-hi-90'].values,\n        label='LGBMRegressor-level-90',\n        color='orange',\n        alpha=0.2\n    )\n    axi.legend()\nfig.savefig('figs/forecast__cross_validation_intervals.png', bbox_inches='tight')\nplt.close()\n\n\n\n\nCreate MLForecast from LightGBMCV\nOnce you’ve found a set of features and parameters that work for your problem you can build a forecast object from it using MLForecast.from_cv, which takes the trained LightGBMCV object and builds an MLForecast object that will use the same features and parameters. Then you can call fit and predict as you normally would.\n\ncv = LightGBMCV(\n    freq=1,\n    lags=[24 * (i+1) for i in range(7)],\n    lag_transforms={\n        48: [(ewm_mean, 0.3)],\n    },\n    num_threads=1,\n    target_transforms=[Differences([24])]\n)\nhist = cv.fit(\n    train,\n    n_windows=2,\n    h=horizon,\n    params={'verbosity': -1},\n)\n\n[LightGBM] [Info] Start training from score 0.084340\n[10] mape: 0.118569\n[20] mape: 0.111506\n[30] mape: 0.107314\n[40] mape: 0.106089\n[50] mape: 0.106630\nEarly stopping at round 50\nUsing best iteration: 40\n\n\n\nfcst = MLForecast.from_cv(cv)\nassert cv.best_iteration_ == fcst.models['LGBMRegressor'].n_estimators\n\n\nfcst.fit(train)\nfcst.predict(horizon)\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\n\n\n\n\n0\nH196\n961\n16.111079\n\n\n1\nH196\n962\n15.711079\n\n\n2\nH196\n963\n15.311079\n\n\n3\nH196\n964\n15.011079\n\n\n4\nH196\n965\n14.711079\n\n\n...\n...\n...\n...\n\n\n187\nH413\n1004\n92.722032\n\n\n188\nH413\n1005\n69.153603\n\n\n189\nH413\n1006\n68.811675\n\n\n190\nH413\n1007\n53.693346\n\n\n191\nH413\n1008\n46.055481\n\n\n\n\n192 rows × 3 columns\n\n\n\n\n\nDynamic features\nWe’re going to use a synthetic dataset from this point onwards to demonstrate some other functionalities regarding external regressors.\n\nseries = generate_daily_series(100, equal_ends=True, n_static_features=2, static_as_categorical=False)\nseries\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nstatic_1\n\n\n\n\n0\nid_00\n2000-10-05\n3.981198\n79\n45\n\n\n1\nid_00\n2000-10-06\n10.327401\n79\n45\n\n\n2\nid_00\n2000-10-07\n17.657474\n79\n45\n\n\n3\nid_00\n2000-10-08\n25.898790\n79\n45\n\n\n4\nid_00\n2000-10-09\n34.494040\n79\n45\n\n\n...\n...\n...\n...\n...\n...\n\n\n26998\nid_99\n2001-05-10\n45.340051\n69\n35\n\n\n26999\nid_99\n2001-05-11\n3.022948\n69\n35\n\n\n27000\nid_99\n2001-05-12\n10.131371\n69\n35\n\n\n27001\nid_99\n2001-05-13\n14.572434\n69\n35\n\n\n27002\nid_99\n2001-05-14\n22.816357\n69\n35\n\n\n\n\n27003 rows × 5 columns\n\n\n\nAs we saw in the previous example, the required columns are the series identifier, time and target. Whatever extra columns you have, like static_0 and static_1 here are considered to be static and are replicated when constructing the features for the next timestamp. You can disable this by passing static_features to MLForecast.preprocess or MLForecast.fit , which will only keep the columns you define there as static. Keep in mind that they will still be used for training, so you’ll have to provide them to MLForecast.predict through the dynamic_dfs argument.\nBy default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the dynamic_dfs argument of MLForecast.predict, which will call pd.DataFrame.merge on each of them in order.\nHere’s an example:\nSuppose that we have a product_id column and we have a catalog for prices based on that product_id and the date.\n\ndynamic_series = series.rename(columns={'static_1': 'product_id'})\nprices_catalog = generate_prices_for_series(dynamic_series)\nprices_catalog\n\n\n\n\n\n\n\n\nds\nproduct_id\nprice\n\n\n\n\n0\n2000-06-09\n1\n0.548814\n\n\n1\n2000-06-10\n1\n0.715189\n\n\n2\n2000-06-11\n1\n0.602763\n\n\n3\n2000-06-12\n1\n0.544883\n\n\n4\n2000-06-13\n1\n0.423655\n\n\n...\n...\n...\n...\n\n\n20180\n2001-05-17\n99\n0.223520\n\n\n20181\n2001-05-18\n99\n0.446104\n\n\n20182\n2001-05-19\n99\n0.044783\n\n\n20183\n2001-05-20\n99\n0.483216\n\n\n20184\n2001-05-21\n99\n0.799660\n\n\n\n\n20185 rows × 3 columns\n\n\n\nAnd you have already merged these prices into your series dataframe.\n\nseries_with_prices = dynamic_series.merge(prices_catalog, how='left')\nseries_with_prices\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nproduct_id\nprice\n\n\n\n\n0\nid_00\n2000-10-05\n3.981198\n79\n45\n0.570826\n\n\n1\nid_00\n2000-10-06\n10.327401\n79\n45\n0.260562\n\n\n2\nid_00\n2000-10-07\n17.657474\n79\n45\n0.274048\n\n\n3\nid_00\n2000-10-08\n25.898790\n79\n45\n0.433878\n\n\n4\nid_00\n2000-10-09\n34.494040\n79\n45\n0.653738\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n26998\nid_99\n2001-05-10\n45.340051\n69\n35\n0.792152\n\n\n26999\nid_99\n2001-05-11\n3.022948\n69\n35\n0.782687\n\n\n27000\nid_99\n2001-05-12\n10.131371\n69\n35\n0.019463\n\n\n27001\nid_99\n2001-05-13\n14.572434\n69\n35\n0.190413\n\n\n27002\nid_99\n2001-05-14\n22.816357\n69\n35\n0.653394\n\n\n\n\n27003 rows × 6 columns\n\n\n\nThis dataframe will be passed to MLForecast.fit (or MLForecast.preprocess), however since the price is dynamic we have to tell that method that only static_0 and product_id are static and we’ll have to update price in every timestep, which basically involves merging the updated features with the prices catalog.\n\ndef even_day(dates):\n    return dates.day % 2 == 0\n\nfcst = MLForecast(\n    models=lgb.LGBMRegressor(n_jobs=1, random_state=0),\n    freq='D',\n    lags=[7],\n    lag_transforms={\n        1: [expanding_mean],\n        7: [(rolling_mean, 14)]\n    },\n    date_features=['dayofweek', 'month', even_day],\n    num_threads=2,\n)\nfcst.fit(series_with_prices, static_features=['unique_id', 'static_0', 'product_id'])\n\nMLForecast(models=[LGBMRegressor], freq=&lt;Day&gt;, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month', &lt;function even_day&gt;], num_threads=2)\n\n\nThe features used for training are stored in MLForecast.ts.features_order_, as you can see price was used for training.\n\nfcst.ts.features_order_\n\n['unique_id',\n 'static_0',\n 'product_id',\n 'price',\n 'lag7',\n 'expanding_mean_lag1',\n 'rolling_mean_lag7_window_size14',\n 'dayofweek',\n 'month',\n 'even_day']\n\n\nSo in order to update the price in each timestep we just call MLForecast.predict with our forecast horizon and pass the prices catalog as a dynamic dataframe.\n\npreds = fcst.predict(7, dynamic_dfs=[prices_catalog])\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\n\n\n\n\n0\nid_00\n2001-05-15\n42.406978\n\n\n1\nid_00\n2001-05-16\n50.076236\n\n\n2\nid_00\n2001-05-17\n1.904567\n\n\n3\nid_00\n2001-05-18\n10.259930\n\n\n4\nid_00\n2001-05-19\n18.727878\n\n\n...\n...\n...\n...\n\n\n695\nid_99\n2001-05-17\n44.266018\n\n\n696\nid_99\n2001-05-18\n1.936728\n\n\n697\nid_99\n2001-05-19\n9.091219\n\n\n698\nid_99\n2001-05-20\n15.262409\n\n\n699\nid_99\n2001-05-21\n22.840666\n\n\n\n\n700 rows × 3 columns\n\n\n\n\n\nCustom predictions\nAs you may have noticed MLForecast.predict can take a before_predict_callback and after_predict_callback. By default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features you can pass them as a list to MLForecast.predict in the dynamic_dfs argument. However, if you want to do something to the input before predicting or do something to the output before it gets used to update the target (and thus the next features that rely on lags), you can pass a function to run at any of these times.\nSuppose that we want to look at our inputs and scale our predictions so that our series are updated with these scaled values. We can achieve that with the following:\n\nfrom IPython.display import display\n\n\ndef inspect_input(new_x):\n    \"\"\"Displays the first row of our input to inspect it\"\"\"\n    print('Inputs:')\n    display(new_x.head(1))\n    return new_x\n\ndef increase_predictions(predictions):\n    \"\"\"Prints the last prediction and increases all of them by 10%.\"\"\"\n    print(f'Prediction:\\n{predictions.tail(1)}\\n')\n    return 1.1 * predictions\n\nAnd now we just pass these functions to MLForecast.predict.\n\nfcst = MLForecast(lgb.LGBMRegressor(), freq='D', lags=[1])\nfcst.fit(series)\n\npreds = fcst.predict(2, before_predict_callback=inspect_input, after_predict_callback=increase_predictions)\npreds\n\nInputs:\nPrediction:\nunique_id\nid_99    30.643253\ndtype: float64\n\nInputs:\nPrediction:\nunique_id\nid_99    41.024064\ndtype: float64\n\n\n\n\n\n\n\n\n\n\nstatic_0\nstatic_1\nlag1\n\n\n\n\n0\n79\n45\n34.862245\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatic_0\nstatic_1\nlag1\n\n\n\n\n0\n79\n45\n46.396346\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\n\n\n\n\n0\nid_00\n2001-05-15\n46.396346\n\n\n1\nid_00\n2001-05-16\n23.651944\n\n\n2\nid_01\n2001-05-15\n14.388954\n\n\n3\nid_01\n2001-05-16\n17.796990\n\n\n4\nid_02\n2001-05-15\n15.640528\n\n\n...\n...\n...\n...\n\n\n195\nid_97\n2001-05-16\n39.849693\n\n\n196\nid_98\n2001-05-15\n8.408627\n\n\n197\nid_98\n2001-05-16\n4.290933\n\n\n198\nid_99\n2001-05-15\n33.707579\n\n\n199\nid_99\n2001-05-16\n45.126470\n\n\n\n\n200 rows × 3 columns"
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html",
    "href": "docs/electricity_peak_forecasting.html",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train a LightGBM model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the MLForecast.cross_validation method to fit the LightGBM model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit LightGBM model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html#introduction",
    "href": "docs/electricity_peak_forecasting.html#introduction",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train a LightGBM model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the MLForecast.cross_validation method to fit the LightGBM model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit LightGBM model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html#libraries",
    "href": "docs/electricity_peak_forecasting.html#libraries",
    "title": "Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have MLForecast already installed. Check this guide for instructions on how to install MLForecast.\nInstall the necessary packages using pip install mlforecast.\nAlso we have to install LightGBM using pip install lightgbm."
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html#load-data",
    "href": "docs/electricity_peak_forecasting.html#load-data",
    "title": "Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to MLForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, read the 2022 historic total demand of the ERCOT market. We processed the original data (available here), by adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nY_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv', parse_dates=['ds'])\nY_df = Y_df.query(\"ds &gt;= '2022-01-01' & ds &lt;= '2022-10-01'\")\n\n\nY_df.plot(x='ds', y='y', figsize=(20, 7))\n\n&lt;AxesSubplot: xlabel='ds'&gt;\n\n\n\n\n\nWe observe that the time series exhibits seasonal patterns. Moreover, the time series contains 6,552 observations, so it is necessary to use computationally efficient methods to deploy them in production."
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html#fit-and-forecast-lighgbm-model",
    "href": "docs/electricity_peak_forecasting.html#fit-and-forecast-lighgbm-model",
    "title": "Detect Demand Peaks",
    "section": "Fit and Forecast LighGBM model",
    "text": "Fit and Forecast LighGBM model\nImport the MLForecast class and the models you need.\n\nimport lightgbm as lgb\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\n\nFirst, instantiate the model and define the parameters.\n\n\n\n\n\n\nTip\n\n\n\nIn this example we are using the default parameters of the lgb.LGBMRegressor model, but you can change them to improve the forecasting performance.\n\n\n\nmodels = [\n    lgb.LGBMRegressor() # you can include more models here\n]\n\nWe fit the model by instantiating a MLForecast object with the following required parameters:\n\nmodels: a list of sklearn-like (fit and predict) models.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\ntarget_transforms: Transformations to apply to the target before computing the features. These are restored at the forecasting step.\nlags: Lags of the target to use as features.\n\n\n# Instantiate MLForecast class as mlf\nmlf = MLForecast(\n    models=models,\n    freq='H', \n    target_transforms=[Differences([24])],\n    lags=range(1, 25)\n)\n\n\n\n\n\n\n\nTip\n\n\n\nIn this example, we are only using differences and lags to produce features. See the full documentation to see all available features.\n\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon window_size as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, the step size between windows is 24 (equal to the window_size). This ensure to only produce one forecast per day.\nAdditionally,\n\nid_col: identifies each time series.\ntime_col: indetifies the temporal column of the time series.\ntarget_col: identifies the column to model.\n\n\ncrossvalidation_df = mlf.cross_validation(\n    data=Y_df,\n    window_size=24,\n    n_windows=30,\n)\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\n\n\n\n\n0\nERCOT\n2022-09-01 00:00:00\n2022-08-31 23:00:00\n45482.471757\n45685.265537\n\n\n1\nERCOT\n2022-09-01 01:00:00\n2022-08-31 23:00:00\n43602.658043\n43779.819515\n\n\n2\nERCOT\n2022-09-01 02:00:00\n2022-08-31 23:00:00\n42284.817342\n42672.470923\n\n\n3\nERCOT\n2022-09-01 03:00:00\n2022-08-31 23:00:00\n41663.156771\n42091.768192\n\n\n4\nERCOT\n2022-09-01 04:00:00\n2022-08-31 23:00:00\n41710.621904\n42481.403168\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window."
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html#peak-detection",
    "href": "docs/electricity_peak_forecasting.html#peak-detection",
    "title": "Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','LGBMRegressor']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['LGBMRegressor'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the LightGBM model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nax.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['LGBMRegressor'], color='green', label=f'Predicted Top-{npeaks}')\nax.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nax.plot(cv_df_day['ds'], cv_df_day['LGBMRegressor'], label='Forecast', color='red')\nax.set(xlabel='Time', ylabel='Load (MW)')\nax.grid()\nax.legend()\nfig.savefig('../figs/electricity_peak_forecasting__predicted_peak.png', bbox_inches='tight')\nplt.close()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, MLForecast and LightGBM can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the n_windows parameter of cross_validation or filtering the Y_df dataset."
  },
  {
    "objectID": "docs/electricity_peak_forecasting.html#next-steps",
    "href": "docs/electricity_peak_forecasting.html#next-steps",
    "title": "Detect Demand Peaks",
    "section": "Next steps",
    "text": "Next steps\nMLForecast and LightGBM in particular are good benchmarking models for peak detection. However, it might be useful to explore further and newer forecasting algorithms or perform hyperparameter optimization."
  },
  {
    "objectID": "docs/quick_start_local.html",
    "href": "docs/quick_start_local.html",
    "title": "Quick start (local)",
    "section": "",
    "text": "The main component of mlforecast is the MLForecast class, which abstracts away:\n\nFeature engineering and model training through MLForecast.fit\nFeature updates and multi step ahead predictions through MLForecast.predict\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/quick_start_local.html#main-concepts",
    "href": "docs/quick_start_local.html#main-concepts",
    "title": "Quick start (local)",
    "section": "",
    "text": "The main component of mlforecast is the MLForecast class, which abstracts away:\n\nFeature engineering and model training through MLForecast.fit\nFeature updates and multi step ahead predictions through MLForecast.predict"
  },
  {
    "objectID": "docs/quick_start_local.html#data-format",
    "href": "docs/quick_start_local.html#data-format",
    "title": "Quick start (local)",
    "section": "Data format",
    "text": "Data format\nThe data is expected to be a pandas dataframe in long format, that is, each row represents an observation of a single serie at a given time, with at least three columns:\n\nid_col: column that identifies each serie.\ntarget_col: column that has the series values at each timestamp.\ntime_col: column that contains the time the series value was observed. These are usually timestamps, but can also be consecutive integers.\n\nHere we present an example using the classic Box & Jenkins airline data, which measures monthly totals of international airline passengers from 1949 to 1960. Source: Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976) Time Series Analysis, Forecasting and Control. Third Edition. Holden-Day. Series G.\n\nimport pandas as pd\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/air-passengers.csv', parse_dates=['ds'])\ndf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAirPassengers\n1949-01-01\n112\n\n\n1\nAirPassengers\n1949-02-01\n118\n\n\n2\nAirPassengers\n1949-03-01\n132\n\n\n3\nAirPassengers\n1949-04-01\n129\n\n\n4\nAirPassengers\n1949-05-01\n121\n\n\n\n\n\n\n\n\ndf['unique_id'].value_counts()\n\nAirPassengers    144\nName: unique_id, dtype: int64\n\n\nHere the unique_id column has the same value for all rows because this is a single time series, you can have multiple time series by stacking them together and having a column that differentiates them.\nWe also have the ds column that contains the timestamps, in this case with a monthly frequency, and the y column that contains the series values in each timestamp."
  },
  {
    "objectID": "docs/quick_start_local.html#modeling",
    "href": "docs/quick_start_local.html#modeling",
    "title": "Quick start (local)",
    "section": "Modeling",
    "text": "Modeling\n\ndf.plot(x='ds', y='y', figsize=(10, 6));\n\n\n\n\nWe can see that the serie has a clear trend, so we can take the first difference, i.e. take each value and subtract the value at the previous month. This can be achieved by passing an mlforecast.target_transforms.Differences([1]) instance to target_transforms.\nWe can then train a linear regression using the value from the same month at the previous year (lag 12) as a feature, this is done by passing lags=[12].\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom sklearn.linear_model import LinearRegression\n\n\nfcst = MLForecast(\n    models=LinearRegression(),\n    freq='MS',  # our serie has a monthly frequency\n    lags=[12],\n    target_transforms=[Differences([1])],\n)\nfcst.fit(df)\n\nMLForecast(models=[LinearRegression], freq=&lt;MonthBegin&gt;, lag_features=['lag12'], date_features=[], num_threads=1)\n\n\nThe previous line computed the features and trained the model, so now we’re ready to compute our forecasts."
  },
  {
    "objectID": "docs/quick_start_local.html#forecasting",
    "href": "docs/quick_start_local.html#forecasting",
    "title": "Quick start (local)",
    "section": "Forecasting",
    "text": "Forecasting\nCompute the forecast for the next 12 months\n\npreds = fcst.predict(12)\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\nLinearRegression\n\n\n\n\n0\nAirPassengers\n1961-01-01\n444.656555\n\n\n1\nAirPassengers\n1961-02-01\n417.470734\n\n\n2\nAirPassengers\n1961-03-01\n446.903046\n\n\n3\nAirPassengers\n1961-04-01\n491.014130\n\n\n4\nAirPassengers\n1961-05-01\n502.622223\n\n\n5\nAirPassengers\n1961-06-01\n568.751465\n\n\n6\nAirPassengers\n1961-07-01\n660.044312\n\n\n7\nAirPassengers\n1961-08-01\n643.343323\n\n\n8\nAirPassengers\n1961-09-01\n540.666687\n\n\n9\nAirPassengers\n1961-10-01\n491.462708\n\n\n10\nAirPassengers\n1961-11-01\n417.095154\n\n\n11\nAirPassengers\n1961-12-01\n461.206238"
  },
  {
    "objectID": "docs/quick_start_local.html#visualize-results",
    "href": "docs/quick_start_local.html#visualize-results",
    "title": "Quick start (local)",
    "section": "Visualize results",
    "text": "Visualize results\nWe can visualize what our prediction looks like.\n\npd.concat([df, preds]).set_index('ds').plot(figsize=(10, 6));\n\n\n\n\nAnd that’s it! You’ve trained a linear regression to predict the air passengers for 1961."
  },
  {
    "objectID": "docs/quick_start_distributed.html",
    "href": "docs/quick_start_distributed.html",
    "title": "Quick start (distributed)",
    "section": "",
    "text": "The main component for distributed training with mlforecast is the DistributedMLForecast class, which abstracts away:\n\nFeature engineering and model training through DistributedMLForecast.fit\nFeature updates and multi step ahead predictions through DistributedMLForecast.predict\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/quick_start_distributed.html#main-concepts",
    "href": "docs/quick_start_distributed.html#main-concepts",
    "title": "Quick start (distributed)",
    "section": "",
    "text": "The main component for distributed training with mlforecast is the DistributedMLForecast class, which abstracts away:\n\nFeature engineering and model training through DistributedMLForecast.fit\nFeature updates and multi step ahead predictions through DistributedMLForecast.predict"
  },
  {
    "objectID": "docs/quick_start_distributed.html#setup",
    "href": "docs/quick_start_distributed.html#setup",
    "title": "Quick start (distributed)",
    "section": "Setup",
    "text": "Setup\nIn order to perform distributed training you need a dask cluster. In this example we’ll use a local cluster but you can replace it with any other type of remote cluster and the processing will take place there.\n\nfrom dask.distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=2, threads_per_worker=1)  # change this to use a remote cluster\nclient = Client(cluster)"
  },
  {
    "objectID": "docs/quick_start_distributed.html#data-format",
    "href": "docs/quick_start_distributed.html#data-format",
    "title": "Quick start (distributed)",
    "section": "Data format",
    "text": "Data format\nThe data is expected to be a dask dataframe in long format, that is, each row represents an observation of a single serie at a given time, with at least three columns:\n\nid_col: column that identifies each serie.\ntarget_col: column that has the series values at each timestamp.\ntime_col: column that contains the time the series value was observed. These are usually timestamps, but can also be consecutive integers.\n\nYou need to make sure that each serie is only in a single partition. You can do so by setting the id_col as the index in dask or with repartitionByRange in spark.\nHere we present an example with synthetic data.\n\nimport dask.dataframe as dd\nfrom mlforecast.utils import generate_daily_series\n\n\nseries = generate_daily_series(100, with_trend=True)\nseries\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nid_00\n2000-01-01\n0.497650\n\n\n1\nid_00\n2000-01-02\n1.554489\n\n\n2\nid_00\n2000-01-03\n2.734311\n\n\n3\nid_00\n2000-01-04\n4.028039\n\n\n4\nid_00\n2000-01-05\n5.366009\n\n\n...\n...\n...\n...\n\n\n26998\nid_99\n2000-06-25\n34.165302\n\n\n26999\nid_99\n2000-06-26\n28.277320\n\n\n27000\nid_99\n2000-06-27\n29.450129\n\n\n27001\nid_99\n2000-06-28\n30.241885\n\n\n27002\nid_99\n2000-06-29\n31.576907\n\n\n\n\n27003 rows × 3 columns\n\n\n\nHere we can see that the index goes from id_00 to id_99, which means we have 100 different series stacked together.\nWe also have the ds column that contains the timestamps, in this case with a daily frequency, and the y column that contains the series values in each timestamp.\nIn order to perform distributed processing and training we need to have these in a dask dataframe, this is typically done loading them directly in a distributed way, for example with dd.read_parquet.\n\nseries_ddf = dd.from_pandas(series.set_index('unique_id'), npartitions=2)  # make sure we split by id\nseries_ddf = series_ddf.map_partitions(lambda part: part.reset_index())  # we can't have an index\nseries_ddf['unique_id'] = series_ddf['unique_id'].astype('str') # categoricals aren't supported at the moment\nseries_ddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\nnpartitions=2\n\n\n\n\n\n\n\nid_00\nobject\ndatetime64[ns]\nfloat64\n\n\nid_49\n...\n...\n...\n\n\nid_99\n...\n...\n...\n\n\n\n\n\nDask Name: assign, 5 graph layers\n\n\nWe now have a dask dataframe with two partitions which will be processed independently in each machine and their outputs will be combined to perform distributed training."
  },
  {
    "objectID": "docs/quick_start_distributed.html#modeling",
    "href": "docs/quick_start_distributed.html#modeling",
    "title": "Quick start (distributed)",
    "section": "Modeling",
    "text": "Modeling\n\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_sample(df, ax):\n    idxs = df['unique_id'].unique()\n    random.seed(0)\n    sample_idxs = random.choices(idxs, k=4)\n    for uid, axi in zip(sample_idxs, ax.flat):\n        df[df['unique_id'].eq(uid)].set_index('ds').plot(ax=axi, title=uid)\n\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), gridspec_kw=dict(hspace=0.5))\nplot_sample(series, ax)\nfig.savefig('../figs/quick_start_distributed__sample.png', bbox_inches='tight')\nplt.close()\n\n\nWe can see that the series have a clear trend, so we can take the first difference, i.e. take each value and subtract the value at the previous month. This can be achieved by passing an mlforecast.target_transforms.Differences([1]) instance to target_transforms.\nWe can then train a LightGBM model using the value from the same day of the week at the previous week (lag 7) as a feature, this is done by passing lags=[7].\n\nfrom mlforecast.distributed import DistributedMLForecast\nfrom mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\nfrom mlforecast.target_transforms import Differences\n\n\nfcst = DistributedMLForecast(\n    models=DaskLGBMForecast(verbosity=-1),\n    freq='D',\n    lags=[7],\n    target_transforms=[Differences([1])],\n)\nfcst.fit(series_ddf)\n\n/home/jose/mambaforge/envs/mlforecast/lib/python3.10/site-packages/lightgbm/dask.py:525: UserWarning: Parameter n_jobs will be ignored.\n  _log_warning(f\"Parameter {param_alias} will be ignored.\")\n\n\nFinding random open ports for workers\n[LightGBM] [Info] Trying to bind port 52367...\n[LightGBM] [Info] Binding port 52367 succeeded\n[LightGBM] [Info] Listening...\n[LightGBM] [Info] Trying to bind port 48789...\n[LightGBM] [Info] Binding port 48789 succeeded\n[LightGBM] [Info] Listening...\n[LightGBM] [Info] Connected to rank 1\n[LightGBM] [Info] Connected to rank 0\n[LightGBM] [Info] Local rank: 0, total number of machines: 2\n[LightGBM] [Info] Local rank: 1, total number of machines: 2\n[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n[LightGBM] [Warning] num_threads is set=1, n_jobs=-1 will be ignored. Current value: num_threads=1\n\n\nDistributedMLForecast(models=[DaskLGBMForecast], freq=&lt;Day&gt;, lag_features=['lag7'], date_features=[], num_threads=1, engine=None)\n\n\nThe previous line computed the features and trained the model, so now we’re ready to compute our forecasts."
  },
  {
    "objectID": "docs/quick_start_distributed.html#forecasting",
    "href": "docs/quick_start_distributed.html#forecasting",
    "title": "Quick start (distributed)",
    "section": "Forecasting",
    "text": "Forecasting\nCompute the forecast for the next 14 days.\n\npreds = fcst.predict(14)\npreds\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nunique_id\nds\nDaskLGBMForecast\n\n\nnpartitions=2\n\n\n\n\n\n\n\nid_00\nobject\ndatetime64[ns]\nfloat64\n\n\nid_49\n...\n...\n...\n\n\nid_99\n...\n...\n...\n\n\n\n\n\nDask Name: map, 17 graph layers\n\n\nThese are returned as a dask dataframe as well. If it’s safe (memory-wise) we can bring them to the main process.\n\nlocal_preds = preds.compute()"
  },
  {
    "objectID": "docs/quick_start_distributed.html#visualize-results",
    "href": "docs/quick_start_distributed.html#visualize-results",
    "title": "Quick start (distributed)",
    "section": "Visualize results",
    "text": "Visualize results\nWe can visualize what our prediction looks like.\n\nimport pandas as pd\n\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), gridspec_kw=dict(hspace=0.5))\nplot_sample(pd.concat([series, local_preds.set_index('unique_id')]), ax)\nfig.savefig('../figs/quick_start_distributed__sample_prediction.png', bbox_inches='tight')\nplt.close()\n\n\nAnd that’s it! You’ve trained a distributed LightGBM model and computed predictions for the next 14 days."
  },
  {
    "objectID": "docs/transfer_learning.html",
    "href": "docs/transfer_learning.html",
    "title": "Transfer Learning",
    "section": "",
    "text": "Transfer learning refers to the process of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding 🚀 achievements in Machine Learning and has many practical applications.\nFor time series forecasting, the technique allows you to get lightning-fast predictions ⚡ bypassing the tradeoff between accuracy and speed (more than 30 times faster than our already fast AutoARIMA for a similar accuracy).\nThis notebook shows how to generate a pre-trained model to forecast new time series never seen by the model.\nTable of Contents\nYou can run these experiments with Google Colab.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/transfer_learning.html#installing-libraries",
    "href": "docs/transfer_learning.html#installing-libraries",
    "title": "Transfer Learning",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n\n!pip install mlforecast statsforecast datasetsforecast\n\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom datasetsforecast.m3 import M3\nfrom datasetsforecast.losses import mae\nfrom mlforecast import MLForecast\nfrom statsforecast import StatsForecast as sf\nfrom statsforecast.utils import AirPassengersDF"
  },
  {
    "objectID": "docs/transfer_learning.html#load-m3-data",
    "href": "docs/transfer_learning.html#load-m3-data",
    "title": "Transfer Learning",
    "section": "Load M3 Data",
    "text": "Load M3 Data\nThe M3 class will automatically download the complete M3 dataset and process it.\nIt return three Dataframes: Y_df contains the values for the target variables, X_df contains exogenous calendar features and S_df contains static features for each time-series. For this example we will only use Y_df.\nIf you want to use your own data just replace Y_df. Be sure to use a long format and have a simmilar structure than our data set.\n\nY_df_M3, _, _ = M3.load(directory='./', group='Monthly')\n\nIn this tutorial we are only using 1_000 series to speed up computations. Remove the filter to use the whole dataset.\n\nsf.plot(Y_df_M3)"
  },
  {
    "objectID": "docs/transfer_learning.html#model-training",
    "href": "docs/transfer_learning.html#model-training",
    "title": "Transfer Learning",
    "section": "Model Training",
    "text": "Model Training\nUsing the MLForecast.fit method you can train a set of models to your dataset. You can modify the hyperparameters of the model to get a better accuracy, in this case we will use the default hyperparameters of lgb.LGBMRegressor.\n\nmodels = [lgb.LGBMRegressor()]\n\nThe MLForecast object has the following parameters:\n\nmodels: a list of sklearn-like (fit and predict) models.\nfreq: a string indicating the frequency of the data. See panda’s available frequencies.\ndifferences: Differences to take of the target before computing the features. These are restored at the forecasting step.\nlags: Lags of the target to use as features.\n\nIn this example, we are only using differences and lags to produce features. See the full documentation to see all available features.\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame Y_df_M3.\n\nfcst = MLForecast(\n    models=models, \n    lags=range(1, 13),\n    freq='M',\n    differences=[1, 12]\n)\nfcst.fit(Y_df_M3, id_col='unique_id', time_col='ds', target_col='y');"
  },
  {
    "objectID": "docs/transfer_learning.html#transfer-m3-to-airpassengers",
    "href": "docs/transfer_learning.html#transfer-m3-to-airpassengers",
    "title": "Transfer Learning",
    "section": "Transfer M3 to AirPassengers",
    "text": "Transfer M3 to AirPassengers\nNow we can tranfr the trained model to forecast AirPassengers with the MLForecast.predict method, we just have to pass the new dataframe to the new_data argument.\n\n# We define the train df. \nY_df = AirPassengersDF.copy()\n\nY_train_df = Y_df[Y_df.ds&lt;='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds&gt;'1959-12-31']   # 12 test\n\n\nY_hat_df = fcst.predict(horizon=12, new_data=Y_train_df)\nY_hat_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\n\n\n\n\n0\n1.0\n1960-01-31\n422.740096\n\n\n1\n1.0\n1960-02-29\n399.480193\n\n\n2\n1.0\n1960-03-31\n458.220289\n\n\n3\n1.0\n1960-04-30\n442.960385\n\n\n4\n1.0\n1960-05-31\n461.700482\n\n\n\n\n\n\n\n\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n\n\nsf.plot(Y_df, Y_hat_df)"
  },
  {
    "objectID": "docs/transfer_learning.html#evaluate-results",
    "href": "docs/transfer_learning.html#evaluate-results",
    "title": "Transfer Learning",
    "section": "Evaluate Results",
    "text": "Evaluate Results\nWe evaluate the forecasts of the pre-trained model with the Mean Absolute Error (mae).\n\\[\n\\qquad MAE = \\frac{1}{Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}|\\qquad\n\\]\n\ny_true = Y_test_df.y.values\ny_hat = Y_hat_df['LGBMRegressor'].values\n\n\nprint('LGBMRegressor     MAE: %0.3f' % mae(y_hat, y_true))\nprint('ETS               MAE: 16.222')\nprint('AutoARIMA         MAE: 18.551')\n\nLGBMRegressor     MAE: 13.560\nETS               MAE: 16.222\nAutoARIMA         MAE: 18.551"
  },
  {
    "objectID": "lgb_cv.html",
    "href": "lgb_cv.html",
    "title": "LightGBMCV",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "lgb_cv.html#example",
    "href": "lgb_cv.html#example",
    "title": "LightGBMCV",
    "section": "Example",
    "text": "Example\nThis shows an example with just 4 series of the M4 dataset. If you want to run it yourself on all of them, you can refer to this notebook.\n\nimport random\n\nfrom datasetsforecast.m4 import M4, M4Info\nfrom fastcore.test import test_eq, test_fail\nfrom mlforecast.target_transforms import Differences\nfrom nbdev import show_doc\nfrom window_ops.ewm import ewm_mean\nfrom window_ops.rolling import rolling_mean, seasonal_rolling_mean\n\n\ngroup = 'Hourly'\nawait M4.async_download('data', group=group)\ndf, *_ = M4.load(directory='data', group=group)\ndf['ds'] = df['ds'].astype('int')\nids = df['unique_id'].unique()\nrandom.seed(0)\nsample_ids = random.choices(ids, k=4)\nsample_df = df[df['unique_id'].isin(sample_ids)]\nsample_df\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n86796\nH196\n1\n11.8\n\n\n86797\nH196\n2\n11.4\n\n\n86798\nH196\n3\n11.1\n\n\n86799\nH196\n4\n10.8\n\n\n86800\nH196\n5\n10.6\n\n\n...\n...\n...\n...\n\n\n325235\nH413\n1004\n99.0\n\n\n325236\nH413\n1005\n88.0\n\n\n325237\nH413\n1006\n47.0\n\n\n325238\nH413\n1007\n41.0\n\n\n325239\nH413\n1008\n34.0\n\n\n\n\n4032 rows × 3 columns\n\n\n\n\ninfo = M4Info[group]\nhorizon = info.horizon\nvalid = sample_df.groupby('unique_id').tail(horizon)\ntrain = sample_df.drop(valid.index)\ntrain.shape, valid.shape\n\n((3840, 3), (192, 3))\n\n\nWhat LightGBMCV does is emulate LightGBM’s cv function where several Boosters are trained simultaneously on different partitions of the data, that is, one boosting iteration is performed on all of them at a time. This allows to have an estimate of the error by iteration, so if we combine this with early stopping we can find the best iteration to train a final model using all the data or even use these individual models’ predictions to compute an ensemble.\nIn order to have a good estimate of the forecasting performance of our model we compute predictions for the whole test period and compute a metric on that. Since this step can slow down training, there’s an eval_every parameter that can be used to control this, that is, if eval_every=10 (the default) every 10 boosting iterations we’re going to compute forecasts for the complete window and report the error.\nWe also have early stopping parameters:\n\nearly_stopping_evals: how many evaluations of the full window should we go without improving to stop training?\nearly_stopping_pct: what’s the minimum percentage improvement we want in these early_stopping_evals in order to keep training?\n\nThis makes the LightGBMCV class a good tool to quickly test different configurations of the model. Consider the following example, where we’re going to try to find out which features can improve the performance of our model. We start just using lags.\n\nstatic_fit_config = dict(\n    n_windows=2,\n    h=horizon,\n    params={'verbose': -1},\n    compute_cv_preds=True,\n)\ncv = LightGBMCV(\n    freq=1,\n    lags=[24 * (i+1) for i in range(7)],  # one week of lags\n)\n\n\n\nLightGBMCV.fit\n\n LightGBMCV.fit (df:pandas.core.frame.DataFrame, n_windows:int, h:int,\n                 id_col:str='unique_id', time_col:str='ds',\n                 target_col:str='y', step_size:Optional[int]=None,\n                 num_iterations:int=100,\n                 params:Optional[Dict[str,Any]]=None,\n                 static_features:Optional[List[str]]=None,\n                 dropna:bool=True, keep_last_n:Optional[int]=None,\n                 eval_every:int=10,\n                 weights:Optional[Sequence[float]]=None,\n                 metric:Union[str,Callable]='mape',\n                 verbose_eval:bool=True, early_stopping_evals:int=2,\n                 early_stopping_pct:float=0.01,\n                 compute_cv_preds:bool=False,\n                 before_predict_callback:Optional[Callable]=None,\n                 after_predict_callback:Optional[Callable]=None,\n                 input_size:Optional[int]=None,\n                 data:Optional[pandas.core.frame.DataFrame]=None,\n                 window_size:Optional[int]=None)\n\nTrain boosters simultaneously and assess their performance on the complete forecasting window.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nSeries data in long format.\n\n\nn_windows\nint\n\nNumber of windows to evaluate.\n\n\nh\nint\n\nForecast horizon.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstep_size\ntyping.Optional[int]\nNone\nStep size between each cross validation window. If None it will be equal to h.\n\n\nnum_iterations\nint\n100\nMaximum number of boosting iterations to run.\n\n\nparams\ntyping.Optional[typing.Dict[str, typing.Any]]\nNone\nParameters to be passed to the LightGBM Boosters.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\neval_every\nint\n10\nNumber of boosting iterations to train before evaluating on the whole forecast window.\n\n\nweights\ntyping.Optional[typing.Sequence[float]]\nNone\nWeights to multiply the metric of each window. If None, all windows have the same weight.\n\n\nmetric\ntyping.Union[str, typing.Callable]\nmape\nMetric used to assess the performance of the models and perform early stopping.\n\n\nverbose_eval\nbool\nTrue\nPrint the metrics of each evaluation.\n\n\nearly_stopping_evals\nint\n2\nMaximum number of evaluations to run without improvement.\n\n\nearly_stopping_pct\nfloat\n0.01\nMinimum percentage improvement in metric value in early_stopping_evals evaluations.\n\n\ncompute_cv_preds\nbool\nFalse\nCompute predictions for each window after finding the best iteration.\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\ninput_size\ntyping.Optional[int]\nNone\nMaximum training samples per serie in each window. If None, will use an expanding window.\n\n\ndata\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data in long format. This argument has been replaced by df and will be removed in a later release.\n\n\nwindow_size\ntyping.Optional[int]\nNone\nForecast horizon. This argument has been replaced by h and will be removed in a later release.\n\n\nReturns\ntyping.List[typing.Tuple[int, float]]\n\nnoqa: ARG002noqa: ARG002\n\n\n\n\nhist = cv.fit(train, **static_fit_config)\n\n[LightGBM] [Info] Start training from score 51.745632\n[10] mape: 0.590690\n[20] mape: 0.251093\n[30] mape: 0.143643\n[40] mape: 0.109723\n[50] mape: 0.102099\n[60] mape: 0.099448\n[70] mape: 0.098349\n[80] mape: 0.098006\n[90] mape: 0.098718\nEarly stopping at round 90\nUsing best iteration: 80\n\n\nBy setting compute_cv_preds we get the predictions from each model on their corresponding validation fold.\n\ncv.cv_preds_\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nBooster\nwindow\n\n\n\n\n0\nH196\n865\n15.5\n15.522924\n0\n\n\n1\nH196\n866\n15.1\n14.985832\n0\n\n\n2\nH196\n867\n14.8\n14.667901\n0\n\n\n3\nH196\n868\n14.4\n14.514592\n0\n\n\n4\nH196\n869\n14.2\n14.035793\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n187\nH413\n956\n59.0\n77.227905\n1\n\n\n188\nH413\n957\n58.0\n80.589641\n1\n\n\n189\nH413\n958\n53.0\n53.986834\n1\n\n\n190\nH413\n959\n38.0\n36.749786\n1\n\n\n191\nH413\n960\n46.0\n36.281225\n1\n\n\n\n\n384 rows × 5 columns\n\n\n\nThe individual models we trained are saved, so calling predict returns the predictions from every model trained.\n\n\n\nLightGBMCV.predict\n\n LightGBMCV.predict (h:int,\n                     dynamic_dfs:Optional[List[pandas.core.frame.DataFrame\n                     ]]=None,\n                     before_predict_callback:Optional[Callable]=None,\n                     after_predict_callback:Optional[Callable]=None,\n                     horizon:Optional[int]=None)\n\nCompute predictions with each of the trained boosters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndynamic_dfs\ntyping.Optional[typing.List[pandas.core.frame.DataFrame]]\nNone\nFuture values of the dynamic features, e.g. prices.\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\nhorizon\ntyping.Optional[int]\nNone\nForecast horizon. This argument has been replaced by h and will be removed in a later release.\n\n\nReturns\nDataFrame\n\nnoqa: ARG002\n\n\n\n\npreds = cv.predict(horizon)\npreds\n\n\n\n\n\n\n\n\nunique_id\nds\nBooster0\nBooster1\n\n\n\n\n0\nH196\n961\n15.670252\n15.848888\n\n\n1\nH196\n962\n15.522924\n15.697399\n\n\n2\nH196\n963\n14.985832\n15.166213\n\n\n3\nH196\n964\n14.985832\n14.723238\n\n\n4\nH196\n965\n14.562152\n14.451092\n\n\n...\n...\n...\n...\n...\n\n\n187\nH413\n1004\n70.695242\n65.917620\n\n\n188\nH413\n1005\n66.216580\n62.615788\n\n\n189\nH413\n1006\n63.896573\n67.848598\n\n\n190\nH413\n1007\n46.922797\n50.981950\n\n\n191\nH413\n1008\n45.006541\n42.752819\n\n\n\n\n192 rows × 4 columns\n\n\n\nWe can average these predictions and evaluate them.\n\ndef evaluate_on_valid(preds):\n    preds = preds.copy()\n    preds['final_prediction'] = preds.drop(columns=['unique_id', 'ds']).mean(1)\n    merged = preds.merge(valid, on=['unique_id', 'ds'])\n    merged['abs_err'] = abs(merged['final_prediction'] - merged['y']) / merged['y']\n    return merged.groupby('unique_id')['abs_err'].mean().mean()\n\n\neval1 = evaluate_on_valid(preds)\neval1\n\n0.11036194712311806\n\n\nNow, since these series are hourly, maybe we can try to remove the daily seasonality by taking the 168th (24 * 7) difference, that is, substract the value at the same hour from one week ago, thus our target will be \\(z_t = y_{t} - y_{t-168}\\). The features will be computed from this target and when we predict they will be automatically re-applied.\n\ncv2 = LightGBMCV(\n    freq=1,\n    target_transforms=[Differences([24 * 7])],\n    lags=[24 * (i+1) for i in range(7)],\n)\nhist2 = cv2.fit(train, **static_fit_config)\n\n[LightGBM] [Info] Start training from score 0.519010\n[10] mape: 0.089024\n[20] mape: 0.090683\n[30] mape: 0.092316\nEarly stopping at round 30\nUsing best iteration: 10\n\n\n\nassert hist2[-1][1] &lt; hist[-1][1]\n\nNice! We achieve a better score in less iterations. Let’s see if this improvement translates to the validation set as well.\n\npreds2 = cv2.predict(horizon)\neval2 = evaluate_on_valid(preds2)\neval2\n\n0.08956665504570135\n\n\n\nassert eval2 &lt; eval1\n\nGreat! Maybe we can try some lag transforms now. We’ll try the seasonal rolling mean that averages the values “every season”, that is, if we set season_length=24 and window_size=7 then we’ll average the value at the same hour for every day of the week.\n\ncv3 = LightGBMCV(\n    freq=1,\n    target_transforms=[Differences([24 * 7])],\n    lags=[24 * (i+1) for i in range(7)],\n    lag_transforms={\n        48: [(seasonal_rolling_mean, 24, 7)],\n    },\n)\nhist3 = cv3.fit(train, **static_fit_config)\n\n[LightGBM] [Info] Start training from score 0.273641\n[10] mape: 0.086724\n[20] mape: 0.088466\n[30] mape: 0.090536\nEarly stopping at round 30\nUsing best iteration: 10\n\n\nSeems like this is helping as well!\n\nassert hist3[-1][1] &lt; hist2[-1][1]\n\nDoes this reflect on the validation set?\n\npreds3 = cv3.predict(horizon)\neval3 = evaluate_on_valid(preds3)\neval3\n\n0.08961279023129345\n\n\nNice! mlforecast also supports date features, but in this case our time column is made from integers so there aren’t many possibilites here. As you can see this allows you to iterate faster and get better estimates of the forecasting performance you can expect from your model.\nIf you’re doing hyperparameter tuning it’s useful to be able to run a couple of iterations, assess the performance, and determine if this particular configuration isn’t promising and should be discarded. For example, optuna has pruners that you can call with your current score and it decides if the trial should be discarded. We’ll now show how to do that.\nSince the CV requires a bit of setup, like the LightGBM datasets and the internal features, we have this setup method.\n\n\n\nLightGBMCV.setup\n\n LightGBMCV.setup (df:pandas.core.frame.DataFrame, n_windows:int, h:int,\n                   id_col:str='unique_id', time_col:str='ds',\n                   target_col:str='y', step_size:Optional[int]=None,\n                   params:Optional[Dict[str,Any]]=None,\n                   static_features:Optional[List[str]]=None,\n                   dropna:bool=True, keep_last_n:Optional[int]=None,\n                   weights:Optional[Sequence[float]]=None,\n                   metric:Union[str,Callable]='mape',\n                   input_size:Optional[int]=None,\n                   data:Optional[pandas.core.frame.DataFrame]=None,\n                   window_size:Optional[int]=None)\n\nInitialize internal data structures to iteratively train the boosters. Use this before calling partial_fit.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nSeries data in long format.\n\n\nn_windows\nint\n\nNumber of windows to evaluate.\n\n\nh\nint\n\nForecast horizon.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstep_size\ntyping.Optional[int]\nNone\nStep size between each cross validation window. If None it will be equal to h.\n\n\nparams\ntyping.Optional[typing.Dict[str, typing.Any]]\nNone\nParameters to be passed to the LightGBM Boosters.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\nweights\ntyping.Optional[typing.Sequence[float]]\nNone\nWeights to multiply the metric of each window. If None, all windows have the same weight.\n\n\nmetric\ntyping.Union[str, typing.Callable]\nmape\nMetric used to assess the performance of the models and perform early stopping.\n\n\ninput_size\ntyping.Optional[int]\nNone\nMaximum training samples per serie in each window. If None, will use an expanding window.\n\n\ndata\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nSeries data in long format. This argument has been replaced by df and will be removed in a later release.\n\n\nwindow_size\ntyping.Optional[int]\nNone\nForecast horizon. This argument has been replaced by h and will be removed in a later release.\n\n\nReturns\nLightGBMCV\n\nCV object with internal data structures for partial_fit.\n\n\n\n\ncv4 = LightGBMCV(\n    freq=1,\n    lags=[24 * (i+1) for i in range(7)],\n)\ncv4.setup(\n    train,\n    n_windows=2,\n    h=horizon,\n    params={'verbose': -1},\n)\n\nLightGBMCV(freq=1, lag_features=['lag24', 'lag48', 'lag72', 'lag96', 'lag120', 'lag144', 'lag168'], date_features=[], num_threads=1, bst_threads=8)\n\n\nOnce we have this we can call partial_fit to only train for some iterations and return the score of the forecast window.\n\n\n\nLightGBMCV.partial_fit\n\n LightGBMCV.partial_fit (num_iterations:int,\n                         before_predict_callback:Optional[Callable]=None,\n                         after_predict_callback:Optional[Callable]=None)\n\nTrain the boosters for some iterations.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_iterations\nint\n\nNumber of boosting iterations to run\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\nReturns\nfloat\n\nWeighted metric after training for num_iterations.\n\n\n\n\nscore = cv4.partial_fit(10)\nscore\n\n[LightGBM] [Info] Start training from score 51.745632\n\n\n0.5906900462828166\n\n\nThis is equal to the first evaluation from our first example.\n\nassert hist[0][1] == score\n\nWe can now use this score to decide if this configuration is promising. If we want to we can train some more iterations.\n\nscore2 = cv4.partial_fit(20)\n\nThis is now equal to our third metric from the first example, since this time we trained for 20 iterations.\n\nassert hist[2][1] == score2\n\n\n\nUsing a custom metric\nThe built-in metrics are MAPE and RMSE, which are computed by serie and then averaged across all series. If you want to do something different or use a different metric entirely, you can define your own metric like the following:\n\ndef weighted_mape(\n    y_true: pd.Series,\n    y_pred: pd.Series,\n    ids: pd.Series,\n    dates: pd.Series,\n):\n    \"\"\"Weighs the MAPE by the magnitude of the series values\"\"\"\n    abs_pct_err = abs(y_true - y_pred) / abs(y_true)\n    mape_by_serie = abs_pct_err.groupby(ids).mean()\n    totals_per_serie = y_pred.groupby(ids).sum()\n    series_weights = totals_per_serie / totals_per_serie.sum()\n    return (mape_by_serie * series_weights).sum()\n\n\n_ = LightGBMCV(\n    freq=1,\n    lags=[24 * (i+1) for i in range(7)],\n).fit(\n    train,\n    n_windows=2,\n    h=horizon,\n    params={'verbose': -1},\n    metric=weighted_mape,\n)\n\n[LightGBM] [Info] Start training from score 51.745632\n[10] weighted_mape: 0.480353\n[20] weighted_mape: 0.218670\n[30] weighted_mape: 0.161706\n[40] weighted_mape: 0.149992\n[50] weighted_mape: 0.149024\n[60] weighted_mape: 0.148496\nEarly stopping at round 60\nUsing best iteration: 60"
  },
  {
    "objectID": "distributed.forecast.html",
    "href": "distributed.forecast.html",
    "title": "DistributedMLForecast",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "distributed.forecast.html#dask",
    "href": "distributed.forecast.html#dask",
    "title": "DistributedMLForecast",
    "section": "Dask",
    "text": "Dask\n\nClient setup\n\nclient = Client(n_workers=2, threads_per_worker=1)\n\nHere we define a client that connects to a dask.distributed.LocalCluster, however it could be any other kind of cluster.\n\n\nData setup\nFor dask, the data must be a dask.dataframe.DataFrame. You need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set num_threads=1 to avoid having nested parallelism.\nThe required input format is the same as for MLForecast, except that it’s a dask.dataframe.DataFrame instead of a pandas.Dataframe.\n\nseries = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\nnpartitions = 10\npartitioned_series = dd.from_pandas(series.set_index('unique_id'), npartitions=npartitions)  # make sure we split by the id_col\npartitioned_series = partitioned_series.map_partitions(lambda df: df.reset_index())\npartitioned_series['unique_id'] = partitioned_series['unique_id'].astype(str)  # can't handle categoricals atm\npartitioned_series\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nstatic_1\n\n\nnpartitions=10\n\n\n\n\n\n\n\n\n\nid_00\nobject\ndatetime64[ns]\nfloat64\nint64\nint64\n\n\nid_10\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n\n\nid_89\n...\n...\n...\n...\n...\n\n\nid_99\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: assign, 5 graph layers\n\n\n\n\nModels\nIn order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using dask. The current implementations are in DaskLGBMForecast and DaskXGBForecast which are just wrappers around the native implementations.\n\nmodels = [DaskXGBForecast(random_state=0), DaskLGBMForecast(random_state=0)]\n\n\n\nTraining\nOnce we have our models we instantiate a DistributedMLForecast object defining our features.\n\nfcst = DistributedMLForecast(\n    models=models,\n    freq='D',\n    lags=[7],\n    lag_transforms={\n        1: [expanding_mean],\n        7: [(rolling_mean, 14)]\n    },\n    date_features=['dayofweek', 'month'],\n    num_threads=1,\n    engine=client,\n)\nfcst\n\nDistributedMLForecast(models=[DaskXGBForecast, DaskLGBMForecast], freq=&lt;Day&gt;, lag_features=['lag7', 'expanding_mean_lag1', 'rolling_mean_lag7_window_size14'], date_features=['dayofweek', 'month'], num_threads=1, engine=&lt;Client: 'tcp://127.0.0.1:42319' processes=2 threads=2, memory=15.48 GiB&gt;)\n\n\nHere where we say that:\n\nOur series have daily frequency.\nWe want to use lag 7 as a feature\nWe want the lag transformations to be:\n\nexpanding mean of the lag 1\nrolling mean of the lag 7 over a window of size 14\n\nWe want to use dayofweek and month as date features.\nWe want to perform the preprocessing and the forecasting steps using 1 thread, because we have 10 partitions and 2 workers.\n\nFrom this point we have two options:\n\nCompute the features and fit our models.\nCompute the features and get them back as a dataframe to do some custom splitting or adding additional features, then training the models.\n\n\n\n1. Using all the data"
  },
  {
    "objectID": "distributed.forecast.html#distributedmlforecast.fit",
    "href": "distributed.forecast.html#distributedmlforecast.fit",
    "title": "DistributedMLForecast",
    "section": "DistributedMLForecast.fit",
    "text": "DistributedMLForecast.fit\n\n DistributedMLForecast.fit (df:~AnyDataFrame, id_col:str='unique_id',\n                            time_col:str='ds', target_col:str='y',\n                            static_features:Optional[List[str]]=None,\n                            dropna:bool=True,\n                            keep_last_n:Optional[int]=None,\n                            data:Optional[~AnyDataFrame]=None)\n\nApply the feature engineering and train the models.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nAnyDataFrame\n\nSeries data in long format.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\ndata\ntyping.Optional[~AnyDataFrame]\nNone\n\n\n\nReturns\nDistributedMLForecast\n\nnoqa: ARG002\n\n\n\nCalling fit on our data computes the features independently for each partition and performs distributed training.\n\nfcst.fit(partitioned_series)\n\n\nForecasting"
  },
  {
    "objectID": "distributed.forecast.html#distributedmlforecast.predict",
    "href": "distributed.forecast.html#distributedmlforecast.predict",
    "title": "DistributedMLForecast",
    "section": "DistributedMLForecast.predict",
    "text": "DistributedMLForecast.predict\n\n DistributedMLForecast.predict (h:int,\n                                dynamic_dfs:Optional[List[pandas.core.fram\n                                e.DataFrame]]=None, before_predict_callbac\n                                k:Optional[Callable]=None, after_predict_c\n                                allback:Optional[Callable]=None,\n                                new_df:Optional[~AnyDataFrame]=None,\n                                horizon:Optional[int]=None,\n                                new_data:Optional[~AnyDataFrame]=None)\n\nCompute the predictions for the next horizon steps.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndynamic_dfs\ntyping.Optional[typing.List[pandas.core.frame.DataFrame]]\nNone\nFuture values of the dynamic features, e.g. prices.\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\nnew_df\ntyping.Optional[~AnyDataFrame]\nNone\nSeries data of new observations for which forecasts are to be generated. This dataframe should have the same structure as the one used to fit the model, including any features and time series data. If new_df is not None, the method will generate forecasts for the new observations.\n\n\nhorizon\ntyping.Optional[int]\nNone\n\n\n\nnew_data\ntyping.Optional[~AnyDataFrame]\nNone\n\n\n\nReturns\nAnyDataFrame\n\nPredictions for each serie and timestep, with one column per model.\n\n\n\nOnce we have our fitted models we can compute the predictions for the next 7 timesteps.\n\npreds = fcst.predict(7)\npreds\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nunique_id\nds\nDaskXGBForecast\nDaskLGBMForecast\n\n\nnpartitions=10\n\n\n\n\n\n\n\n\nid_00\nobject\ndatetime64[ns]\nfloat64\nfloat64\n\n\nid_10\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\nid_89\n...\n...\n...\n...\n\n\nid_99\n...\n...\n...\n...\n\n\n\n\n\nDask Name: map, 17 graph layers\n\n\n\n2. Preprocess and train\nIf we only want to perform the preprocessing step we call preprocess with our data."
  },
  {
    "objectID": "distributed.forecast.html#distributedmlforecast.preprocess",
    "href": "distributed.forecast.html#distributedmlforecast.preprocess",
    "title": "DistributedMLForecast",
    "section": "DistributedMLForecast.preprocess",
    "text": "DistributedMLForecast.preprocess\n\n DistributedMLForecast.preprocess (df:~AnyDataFrame,\n                                   id_col:str='unique_id',\n                                   time_col:str='ds', target_col:str='y', \n                                   static_features:Optional[List[str]]=Non\n                                   e, dropna:bool=True,\n                                   keep_last_n:Optional[int]=None,\n                                   data:Optional[~AnyDataFrame]=None)\n\nAdd the features to data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nAnyDataFrame\n\nSeries data in long format.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\ndata\ntyping.Optional[~AnyDataFrame]\nNone\n\n\n\nReturns\nAnyDataFrame\n\nnoqa: ARG002\n\n\n\n\nfeatures_ddf = fcst.preprocess(partitioned_series)\nfeatures_ddf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nstatic_1\nlag7\nexpanding_mean_lag1\nrolling_mean_lag7_window_size14\n\n\n\n\n20\nid_00\n2000-10-25\n49.766844\n79\n45\n50.694639\n25.001367\n26.320060\n\n\n21\nid_00\n2000-10-26\n3.918347\n79\n45\n3.887780\n26.180675\n26.313387\n\n\n22\nid_00\n2000-10-27\n9.437778\n79\n45\n11.512774\n25.168751\n26.398056\n\n\n23\nid_00\n2000-10-28\n17.923574\n79\n45\n18.038498\n24.484796\n26.425272\n\n\n24\nid_00\n2000-10-29\n26.754645\n79\n45\n24.222859\n24.211411\n26.305563\n\n\n\n\n\n\n\nThis is useful if we want to inspect the data the model will be trained. If we do this we must manually train our models and add a local version of them to the models_ attribute.\n\nX, y = features_ddf.drop(columns=['unique_id', 'ds', 'y']), features_ddf['y']\nmodel = DaskXGBForecast(random_state=0).fit(X, y)\nfcst.models_ = {'DaskXGBForecast': model.model_}\nfcst.predict(7)\n\n\nDynamic features\nBy default the predict method repeats the static features and updates the transformations and the date features. If you have dynamic features like prices or a calendar with holidays you can pass them as a list to the dynamic_dfs argument of DistributedMLForecast.predict, which will call pd.DataFrame.merge on each of them in order.\nHere’s an example:\nSuppose that we have a product_id column and we have a catalog for prices based on that product_id and the date.\n\ndynamic_series = series.rename(columns={'static_1': 'product_id'})\nprices_catalog = generate_prices_for_series(dynamic_series)\nprices_catalog\n\n\n\n\n\n\n\n\nds\nproduct_id\nprice\n\n\n\n\n0\n2000-06-09\n1\n0.548814\n\n\n1\n2000-06-10\n1\n0.715189\n\n\n2\n2000-06-11\n1\n0.602763\n\n\n3\n2000-06-12\n1\n0.544883\n\n\n4\n2000-06-13\n1\n0.423655\n\n\n...\n...\n...\n...\n\n\n20180\n2001-05-17\n99\n0.223520\n\n\n20181\n2001-05-18\n99\n0.446104\n\n\n20182\n2001-05-19\n99\n0.044783\n\n\n20183\n2001-05-20\n99\n0.483216\n\n\n20184\n2001-05-21\n99\n0.799660\n\n\n\n\n20185 rows × 3 columns\n\n\n\nAnd you have already merged these prices into your series dataframe.\n\ndynamic_series = partitioned_series.rename(columns={'static_1': 'product_id'})\ndynamic_series = dynamic_series\nseries_with_prices = dynamic_series.merge(prices_catalog, how='left')\nseries_with_prices.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nstatic_0\nproduct_id\nprice\n\n\n\n\n0\nid_00\n2000-10-05\n3.981198\n79\n45\n0.570826\n\n\n1\nid_00\n2000-10-06\n10.327401\n79\n45\n0.260562\n\n\n2\nid_00\n2000-10-07\n17.657474\n79\n45\n0.274048\n\n\n3\nid_00\n2000-10-08\n25.898790\n79\n45\n0.433878\n\n\n4\nid_00\n2000-10-09\n34.494040\n79\n45\n0.653738\n\n\n\n\n\n\n\nThis dataframe will be passed to DistributedMLForecast.fit (or DistributedMLForecast.preprocess), however since the price is dynamic we have to tell that method that only static_0 and product_id are static and we’ll have to update price in every timestep, which basically involves merging the updated features with the prices catalog.\n\nfcst = DistributedMLForecast(\n    models,\n    freq='D',\n    lags=[7],\n    lag_transforms={\n        1: [expanding_mean],\n        7: [(rolling_mean, 14)]\n    },\n    date_features=['dayofweek', 'month'],\n    num_threads=1,\n)\nseries_with_prices = series_with_prices\nfcst.fit(\n    series_with_prices,\n    static_features=['static_0', 'product_id'],\n)\n\nSo in order to update the price in each timestep we just call DistributedMLForecast.predict with our forecast horizon and pass the prices catalog as a dynamic dataframe.\n\npreds = fcst.predict(7, dynamic_dfs=[prices_catalog])\npreds.compute()\n\n\n\n\n\n\n\n\nunique_id\nds\nDaskXGBForecast\nDaskLGBMForecast\n\n\n\n\n0\nid_00\n2001-05-15\n42.404003\n43.094384\n\n\n1\nid_00\n2001-05-16\n50.457447\n49.880064\n\n\n2\nid_00\n2001-05-17\n2.044398\n1.938665\n\n\n3\nid_00\n2001-05-18\n10.102403\n10.250496\n\n\n4\nid_00\n2001-05-19\n18.245543\n18.473560\n\n\n...\n...\n...\n...\n...\n\n\n72\nid_99\n2001-05-17\n43.536346\n44.494822\n\n\n73\nid_99\n2001-05-18\n2.063584\n2.093080\n\n\n74\nid_99\n2001-05-19\n9.016212\n9.148367\n\n\n75\nid_99\n2001-05-20\n15.630316\n15.048958\n\n\n76\nid_99\n2001-05-21\n22.420233\n23.037681\n\n\n\n\n700 rows × 4 columns\n\n\n\n\n\nCustom predictions\nIf you want to do something like scaling the predictions you can define a function and pass it to DistributedMLForecast.predict as described in Custom predictions.\n\nCross validation\nRefer to MLForecast.cross_validation."
  },
  {
    "objectID": "distributed.forecast.html#distributedmlforecast.cross_validation",
    "href": "distributed.forecast.html#distributedmlforecast.cross_validation",
    "title": "DistributedMLForecast",
    "section": "DistributedMLForecast.cross_validation",
    "text": "DistributedMLForecast.cross_validation\n\n DistributedMLForecast.cross_validation (df:~AnyDataFrame, n_windows:int,\n                                         h:int, id_col:str='unique_id',\n                                         time_col:str='ds',\n                                         target_col:str='y',\n                                         step_size:Optional[int]=None, sta\n                                         tic_features:Optional[List[str]]=\n                                         None, dropna:bool=True,\n                                         keep_last_n:Optional[int]=None,\n                                         refit:bool=True, before_predict_c\n                                         allback:Optional[Callable]=None, \n                                         after_predict_callback:Optional[C\n                                         allable]=None,\n                                         input_size:Optional[int]=None, da\n                                         ta:Optional[~AnyDataFrame]=None,\n                                         window_size:Optional[int]=None)\n\nPerform time series cross validation. Creates n_windows splits where each window has h test periods, trains the models, computes the predictions and merges the actuals.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nAnyDataFrame\n\nSeries data in long format.\n\n\nn_windows\nint\n\nNumber of windows to evaluate.\n\n\nh\nint\n\nNumber of test periods in each window.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nstep_size\ntyping.Optional[int]\nNone\nStep size between each cross validation window. If None it will be equal to h.\n\n\nstatic_features\ntyping.Optional[typing.List[str]]\nNone\nNames of the features that are static and will be repeated when forecasting.\n\n\ndropna\nbool\nTrue\nDrop rows with missing values produced by the transformations.\n\n\nkeep_last_n\ntyping.Optional[int]\nNone\nKeep only these many records from each serie for the forecasting step. Can save time and memory if your features allow it.\n\n\nrefit\nbool\nTrue\nRetrain model for each cross validation window.If False, the models are trained at the beginning and then used to predict each window.\n\n\nbefore_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the features before computing the predictions. This function will take the input dataframe that will be passed to the model for predicting and should return a dataframe with the same structure. The series identifier is on the index.\n\n\nafter_predict_callback\ntyping.Optional[typing.Callable]\nNone\nFunction to call on the predictions before updating the targets. This function will take a pandas Series with the predictions and should return another one with the same structure. The series identifier is on the index.\n\n\ninput_size\ntyping.Optional[int]\nNone\nMaximum training samples per serie in each window. If None, will use an expanding window.\n\n\ndata\ntyping.Optional[~AnyDataFrame]\nNone\n\n\n\nwindow_size\ntyping.Optional[int]\nNone\n\n\n\nReturns\nAnyDataFrame\n\nnoqa: ARG002noqa: ARG002\n\n\n\n\nfcst = DistributedMLForecast(\n    models=[DaskLGBMForecast(), DaskXGBForecast()],\n    freq='D',\n    lags=[7],\n    lag_transforms={\n        1: [expanding_mean],\n        7: [(rolling_mean, 14)]\n    },\n    date_features=['dayofweek', 'month'],\n    num_threads=1,\n)\n\n\nn_windows = 2\nwindow_size = 14\n\ncv_results = fcst.cross_validation(\n    partitioned_series,\n    n_windows,\n    window_size,\n)\ncv_results\n\nWe can aggregate these by date to get a rough estimate of how our model is doing.\n\nagg_results = cv_results_df.drop(columns='cutoff').groupby('ds').mean()\nagg_results.head()\n\n\n\n\n\n\n\n\nDaskLGBMForecast\nDaskXGBForecast\ny\n\n\nds\n\n\n\n\n\n\n\n2001-04-17\n16.195230\n16.168709\n16.123231\n\n\n2001-04-18\n15.145318\n15.135734\n15.213920\n\n\n2001-04-19\n17.149119\n17.087150\n16.985699\n\n\n2001-04-20\n18.002781\n18.045092\n18.068340\n\n\n2001-04-21\n18.136612\n18.142144\n18.200609\n\n\n\n\n\n\n\nWe can also compute the error for each model.\n\ndef mse_from_dask_dataframe(ddf):\n    mses = {}\n    for model_name in ddf.columns.drop(['unique_id', 'ds', 'y', 'cutoff']):\n        mses[model_name] = (ddf['y'] - ddf[model_name]).pow(2).mean()\n    return client.gather(client.compute(mses))\n\n{k: round(v, 2) for k, v in mse_from_dask_dataframe(cv_results).items()}\n\n{'DaskLGBMForecast': 0.92, 'DaskXGBForecast': 0.86}\n\n\n\nclient.close()"
  },
  {
    "objectID": "distributed.forecast.html#spark",
    "href": "distributed.forecast.html#spark",
    "title": "DistributedMLForecast",
    "section": "Spark",
    "text": "Spark\n\nSession setup\n\nfrom pyspark.sql import SparkSession\n\n\nspark = (\n    SparkSession.builder.appName(\"MyApp\")\n    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.10.2\")\n    .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n    .getOrCreate()\n)\n\n\n\nData setup\nFor spark, the data must be a pyspark DataFrame. You need to make sure that each time serie is only in one partition (which you can do using repartitionByRange, for example) and it is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set num_threads=1 to avoid having nested parallelism.\nThe required input format is the same as for MLForecast, i.e. it should have at least an id column, a time column and a target column.\n\nnumPartitions = 4\nseries = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\nspark_series = spark.createDataFrame(series).repartitionByRange(numPartitions, 'unique_id')\n\n\n\nModels\nIn order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using spark. The current implementations are in SparkLGBMForecast and SparkXGBForecast which are just wrappers around the native implementations.\n\nfrom mlforecast.distributed.models.spark.lgb import SparkLGBMForecast\n\nmodels = [SparkLGBMForecast()]\ntry:\n    from xgboost.spark import SparkXGBRegressor\n    from mlforecast.distributed.models.spark.xgb import SparkXGBForecast\n    models.append(SparkXGBForecast())\nexcept ModuleNotFoundError:  # py &lt; 38\n    pass\n\n\n\nTraining\n\nfcst = DistributedMLForecast(\n    models,\n    freq='D',\n    lags=[1],\n    lag_transforms={\n        1: [expanding_mean]\n    },\n    date_features=['dayofweek'],\n)\nfcst.fit(\n    spark_series,\n    static_features=['static_0', 'static_1'],\n)\n\n\n\nForecasting\n\npreds = fcst.predict(14)\n\n\npreds.toPandas()\n\n                                                                                \n\n\n\n\n\n\n\n\n\nunique_id\nds\nSparkLGBMForecast\nSparkXGBForecast\n\n\n\n\n0\nid_00\n2001-05-15\n42.213984\n42.305004\n\n\n1\nid_00\n2001-05-16\n49.718021\n50.262386\n\n\n2\nid_00\n2001-05-17\n1.306248\n1.912686\n\n\n3\nid_00\n2001-05-18\n10.060104\n10.240939\n\n\n4\nid_00\n2001-05-19\n18.070785\n18.265749\n\n\n...\n...\n...\n...\n...\n\n\n1395\nid_99\n2001-05-24\n43.426901\n43.780163\n\n\n1396\nid_99\n2001-05-25\n1.361680\n2.097803\n\n\n1397\nid_99\n2001-05-26\n8.787283\n8.593580\n\n\n1398\nid_99\n2001-05-27\n15.551965\n15.622238\n\n\n1399\nid_99\n2001-05-28\n22.518518\n22.943216\n\n\n\n\n1400 rows × 4 columns\n\n\n\n\n\nCross validation\n\ncv_res = fcst.cross_validation(\n    spark_series,\n    n_windows=2,\n    window_size=14,\n).toPandas()\n\n\ncv_res\n\n\n\n\n\n\n\n\nunique_id\nds\nSparkLGBMForecast\nSparkXGBForecast\ncutoff\ny\n\n\n\n\n0\nid_17\n2001-04-30\n31.467849\n31.676336\n2001-04-16\n30.832464\n\n\n1\nid_07\n2001-04-17\n1.015429\n1.039312\n2001-04-16\n1.034871\n\n\n2\nid_06\n2001-04-29\n21.133919\n1.368022\n2001-04-16\n0.944155\n\n\n3\nid_11\n2001-04-17\n57.069013\n57.591526\n2001-04-16\n57.406090\n\n\n4\nid_12\n2001-04-27\n7.965585\n7.741258\n2001-04-16\n8.498222\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2795\nid_96\n2001-05-12\n9.069598\n8.925149\n2001-04-30\n7.983343\n\n\n2796\nid_84\n2001-05-04\n10.474623\n9.959846\n2001-04-30\n10.683266\n\n\n2797\nid_87\n2001-05-07\n2.162316\n2.065432\n2001-04-30\n1.277810\n\n\n2798\nid_80\n2001-05-11\n22.679552\n20.547785\n2001-04-30\n19.823192\n\n\n2799\nid_90\n2001-05-08\n40.225448\n40.293419\n2001-04-30\n39.215204\n\n\n\n\n2800 rows × 6 columns\n\n\n\n\nspark.stop()"
  },
  {
    "objectID": "distributed.forecast.html#ray",
    "href": "distributed.forecast.html#ray",
    "title": "DistributedMLForecast",
    "section": "Ray",
    "text": "Ray\n\nSession setup\n\nimport ray\nfrom ray.cluster_utils import Cluster\n\n\nray_cluster = Cluster(\n    initialize_head=True,\n    head_node_args={\"num_cpus\": 2}\n)\nray.init(address=ray_cluster.address, ignore_reinit_error=True)\n# add mock node to simulate a cluster\nmock_node = ray_cluster.add_node(num_cpus=2)\n\n\n\nData setup\nFor ray, the data must be a ray DataFrame. It is recommended that you have as many partitions as you have workers. If you have more partitions than workers make sure to set num_threads=1 to avoid having nested parallelism.\nThe required input format is the same as for MLForecast, i.e. it should have at least an id column, a time column and a target column.\n\nseries = generate_daily_series(100, n_static_features=2, equal_ends=True, static_as_categorical=False)\n# we need noncategory unique_id\nseries['unique_id'] = series['unique_id'].astype(str)\nray_series = ray.data.from_pandas(series)\n\n\n\nModels\nThe ray integration allows to include lightgbm (RayLGBMRegressor), and xgboost (RayXGBRegressor).\n\nfrom mlforecast.distributed.models.ray.lgb import RayLGBMForecast\nfrom mlforecast.distributed.models.ray.xgb import RayXGBForecast\n\nmodels = [\n    RayLGBMForecast(),\n    RayXGBForecast(),\n]\n\n\n\nTraining\nTo control the number of partitions to use using Ray, we have to include num_partitions to DistributedMLForecast.\n\nnum_partitions = 4\n\n\nfcst = DistributedMLForecast(\n    models,\n    freq='D',\n    lags=[1],\n    lag_transforms={\n        1: [expanding_mean]\n    },\n    date_features=['dayofweek'],\n    num_partitions=num_partitions, # Use num_partitions to reduce overhead\n)\nfcst.fit(\n    ray_series,\n    static_features=['static_0', 'static_1'],\n)\n\n\n\nForecasting\n\npreds = fcst.predict(14)\n\n\npreds.to_pandas()\n\n\n\n\n\n\n\n\nunique_id\nds\nRayLGBMForecast\nRayXGBForecast\n\n\n\n\n0\nid_00\n2001-05-15\n42.213984\n41.992321\n\n\n1\nid_00\n2001-05-16\n49.718021\n50.999878\n\n\n2\nid_00\n2001-05-17\n1.306248\n1.712625\n\n\n3\nid_00\n2001-05-18\n10.060104\n10.157331\n\n\n4\nid_00\n2001-05-19\n18.070785\n18.163649\n\n\n...\n...\n...\n...\n...\n\n\n1395\nid_99\n2001-05-24\n43.426901\n42.060478\n\n\n1396\nid_99\n2001-05-25\n1.361680\n2.587303\n\n\n1397\nid_99\n2001-05-26\n8.787283\n8.652343\n\n\n1398\nid_99\n2001-05-27\n15.551965\n15.278493\n\n\n1399\nid_99\n2001-05-28\n22.518518\n22.898369\n\n\n\n\n1400 rows × 4 columns\n\n\n\n\n\nCross validation\n\ncv_res = fcst.cross_validation(\n    ray_series,\n    n_windows=2,\n    window_size=14,\n).to_pandas()\n\n\ncv_res\n\n\n\n\n\n\n\n\nunique_id\nds\nRayLGBMForecast\nRayXGBForecast\ncutoff\ny\n\n\n\n\n0\nid_00\n2001-04-17\n41.395948\n41.968201\n2001-04-16\n40.499332\n\n\n1\nid_00\n2001-04-18\n50.004670\n50.191704\n2001-04-16\n50.888323\n\n\n2\nid_00\n2001-04-19\n1.821105\n1.978645\n2001-04-16\n0.121812\n\n\n3\nid_00\n2001-04-20\n10.266459\n10.211697\n2001-04-16\n10.987977\n\n\n4\nid_00\n2001-04-21\n18.285400\n17.944368\n2001-04-16\n16.370385\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2795\nid_69\n2001-05-07\n2.151752\n1.971745\n2001-04-30\n0.768383\n\n\n2796\nid_82\n2001-05-09\n29.733492\n29.844685\n2001-04-30\n29.584063\n\n\n2797\nid_80\n2001-05-03\n14.978611\n15.564403\n2001-04-30\n14.888339\n\n\n2798\nid_28\n2001-05-10\n18.204017\n16.644882\n2001-04-30\n16.512652\n\n\n2799\nid_93\n2001-05-01\n29.756173\n29.035599\n2001-04-30\n29.340027\n\n\n\n\n2800 rows × 6 columns\n\n\n\n\nray.shutdown()"
  }
]